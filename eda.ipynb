{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPLORATORY DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 22:01:01.492805: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-12 22:01:01.637434: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-12 22:01:01.722735: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-12 22:01:01.753189: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-12 22:01:01.897107: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-12 22:01:03.923156: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test data from Tox21 dataset\n",
    "x_train = pd.read_csv('data/tox21_dense_train.csv')\n",
    "y_train = pd.read_csv('data/tox21_labels_train.csv')\n",
    "\n",
    "x_test = pd.read_csv('data/tox21_dense_test.csv')\n",
    "y_test = pd.read_csv('data/tox21_labels_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>AW</th>\n",
       "      <th>AWeight</th>\n",
       "      <th>Arto</th>\n",
       "      <th>BertzCT</th>\n",
       "      <th>Chi0</th>\n",
       "      <th>Chi1</th>\n",
       "      <th>Chi10</th>\n",
       "      <th>Chi2</th>\n",
       "      <th>Chi3</th>\n",
       "      <th>...</th>\n",
       "      <th>NR.AR.LBD</th>\n",
       "      <th>NR.Aromatase</th>\n",
       "      <th>NR.ER</th>\n",
       "      <th>NR.ER.LBD</th>\n",
       "      <th>NR.PPAR.gamma</th>\n",
       "      <th>SR.ARE</th>\n",
       "      <th>SR.ATAD5</th>\n",
       "      <th>SR.HSE</th>\n",
       "      <th>SR.MMP</th>\n",
       "      <th>SR.p53</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NCGC00178831-03</td>\n",
       "      <td>5.436720e+07</td>\n",
       "      <td>13.053</td>\n",
       "      <td>2.176</td>\n",
       "      <td>3.194</td>\n",
       "      <td>23.112</td>\n",
       "      <td>15.868</td>\n",
       "      <td>1.496</td>\n",
       "      <td>15.127</td>\n",
       "      <td>12.592</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NCGC00166114-03</td>\n",
       "      <td>1.268818e+07</td>\n",
       "      <td>22.123</td>\n",
       "      <td>2.065</td>\n",
       "      <td>3.137</td>\n",
       "      <td>21.033</td>\n",
       "      <td>13.718</td>\n",
       "      <td>1.937</td>\n",
       "      <td>13.187</td>\n",
       "      <td>11.951</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NCGC00263563-01</td>\n",
       "      <td>3.076932e+06</td>\n",
       "      <td>13.085</td>\n",
       "      <td>2.154</td>\n",
       "      <td>3.207</td>\n",
       "      <td>46.896</td>\n",
       "      <td>29.958</td>\n",
       "      <td>3.806</td>\n",
       "      <td>30.105</td>\n",
       "      <td>25.569</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NCGC00013058-02</td>\n",
       "      <td>7.168569e+07</td>\n",
       "      <td>12.832</td>\n",
       "      <td>2.029</td>\n",
       "      <td>3.380</td>\n",
       "      <td>51.086</td>\n",
       "      <td>32.045</td>\n",
       "      <td>1.806</td>\n",
       "      <td>29.090</td>\n",
       "      <td>21.603</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NCGC00167516-01</td>\n",
       "      <td>7.989702e+06</td>\n",
       "      <td>12.936</td>\n",
       "      <td>2.124</td>\n",
       "      <td>3.573</td>\n",
       "      <td>70.295</td>\n",
       "      <td>46.402</td>\n",
       "      <td>3.604</td>\n",
       "      <td>42.132</td>\n",
       "      <td>32.570</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 814 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0            AW  AWeight   Arto  BertzCT    Chi0    Chi1  \\\n",
       "0  NCGC00178831-03  5.436720e+07   13.053  2.176    3.194  23.112  15.868   \n",
       "1  NCGC00166114-03  1.268818e+07   22.123  2.065    3.137  21.033  13.718   \n",
       "2  NCGC00263563-01  3.076932e+06   13.085  2.154    3.207  46.896  29.958   \n",
       "3  NCGC00013058-02  7.168569e+07   12.832  2.029    3.380  51.086  32.045   \n",
       "4  NCGC00167516-01  7.989702e+06   12.936  2.124    3.573  70.295  46.402   \n",
       "\n",
       "   Chi10    Chi2    Chi3  ...  NR.AR.LBD  NR.Aromatase  NR.ER  NR.ER.LBD  \\\n",
       "0  1.496  15.127  12.592  ...        NaN           NaN    NaN        NaN   \n",
       "1  1.937  13.187  11.951  ...        NaN           NaN    NaN        NaN   \n",
       "2  3.806  30.105  25.569  ...        NaN           NaN    NaN        NaN   \n",
       "3  1.806  29.090  21.603  ...        NaN           NaN    NaN        NaN   \n",
       "4  3.604  42.132  32.570  ...        NaN           NaN    NaN        NaN   \n",
       "\n",
       "   NR.PPAR.gamma  SR.ARE  SR.ATAD5  SR.HSE  SR.MMP  SR.p53  \n",
       "0            NaN     NaN       NaN     0.0     NaN     NaN  \n",
       "1            NaN     NaN       NaN     0.0     NaN     NaN  \n",
       "2            NaN     NaN       NaN     0.0     NaN     NaN  \n",
       "3            NaN     NaN       NaN     1.0     NaN     NaN  \n",
       "4            NaN     NaN       NaN     NaN     NaN     NaN  \n",
       "\n",
       "[5 rows x 814 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.merge(x_train, y_train, on='Unnamed: 0', how='inner')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (12060, 802)\n",
      "Train labels shape: (12060, 13)\n",
      "Overall train data shape: (12060, 814)\n"
     ]
    }
   ],
   "source": [
    "print('Train data shape:', x_train.shape)\n",
    "print('Train labels shape:', y_train.shape)\n",
    "print('Overall train data shape:', df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_cv, df_final_train = train_test_split(df_train, test_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PRE-FILTERING SHAPE ===\n",
      "Cross-validation data shape: (2412, 814)\n",
      "Final train data shape: (9648, 814)\n"
     ]
    }
   ],
   "source": [
    "print(\"=== PRE-FILTERING SHAPE ===\")\n",
    "print('Cross-validation data shape:', df_cv.shape)\n",
    "print('Final train data shape:', df_final_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate label density for each compound\n",
    "df_cv['label_density'] = df_cv.iloc[:, -12:].notna().sum(axis=1)\n",
    "\n",
    "# now we keep only compounds with at least 8 labels in the cross validation set\n",
    "df_cv_filtered = df_cv[df_cv['label_density'] >= 8]\n",
    "\n",
    "# move the rest to the final training set\n",
    "df_final_train = pd.concat([df_final_train, df_cv[df_cv['label_density'] < 8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== POST-FILTERING SHAPE ===\n",
      "Cross-validation data shape: (1732, 815)\n",
      "Final train data shape: (10328, 815)\n"
     ]
    }
   ],
   "source": [
    "print(\"=== POST-FILTERING SHAPE ===\")\n",
    "print('Cross-validation data shape:', df_cv_filtered.shape)\n",
    "print('Final train data shape:', df_final_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition and Training Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MultiTask Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# Define model training code\n",
    "class MultiTaskToxicityModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=512, num_tasks=12, dropout_rate=0.5):\n",
    "        super(MultiTaskToxicityModel, self).__init__()\n",
    "\n",
    "        # Hidden layers\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        # dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # output layers, and we have one for each task\n",
    "        self.output_layers = nn.ModuleList([nn.Linear(hidden_dim, 1) for _ in range(num_tasks)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # pass through hidden layers with relu activations and dropout\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # pass through output layers\n",
    "        outputs = [torch.sigmoid(output_layer(x)) for output_layer in self.output_layers]\n",
    "\n",
    "        # Concatenate outputs for each task along the batch dimension\n",
    "        return torch.cat(outputs, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "def masked_bce_loss(y_pred, y_true, mask):\n",
    "    # clamp predictions to avoid log(0)\n",
    "    eps = 1e-7\n",
    "    y_pred = torch.clamp(y_pred, eps, 1 - eps)\n",
    "\n",
    "    # compute individual binary crossentropy loss\n",
    "    bce_loss = - (y_true * torch.log(y_pred) + (1 - y_true) * torch.log(1 - y_pred))\n",
    "    \n",
    "    # apply the mask\n",
    "    masked_bce_loss = bce_loss * mask\n",
    "\n",
    "    # return the mean loss\n",
    "    return masked_bce_loss.sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import Adam\n",
    "\n",
    "def train_model(model, optimizer, X_train, y_train, epochs, batch_size):\n",
    "    # Convert data to tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "\n",
    "    # Create a binary mask (1 for valid labels, 0 for NaNs in y_train)\n",
    "    mask_tensor = torch.isnan(y_train_tensor).logical_not().float()\n",
    "\n",
    "    # Replace NaNs in y_train_tensor with zeros (they won't contribute to loss due to masking)\n",
    "    y_train_tensor[torch.isnan(y_train_tensor)] = 0\n",
    "\n",
    "    # Create a TensorDataset and DataLoader\n",
    "    train_data = TensorDataset(X_train_tensor, y_train_tensor, mask_tensor)\n",
    "    train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Begin training loop\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0  # Accumulate loss over all batches in this epoch\n",
    "\n",
    "        for i, (X_batch, y_batch, mask_batch) in enumerate(train_loader):\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            y_pred = model(X_batch)\n",
    "\n",
    "            # Calculate masked binary cross-entropy loss\n",
    "            loss = masked_bce_loss(y_pred, y_batch, mask_batch)  # Pass mask_batch, not mask_tensor\n",
    "            total_loss += loss.item()  # Accumulate batch loss\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Print average loss for the epoch\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss}\")\n",
    "\n",
    "    # Return the trained model\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training And Initialize Training Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "num_tasks = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 59.99110905545934\n",
      "Epoch 2/10, Loss: 51.54362818428216\n",
      "Epoch 3/10, Loss: 50.240195173658535\n",
      "Epoch 4/10, Loss: 49.079479768425635\n",
      "Epoch 5/10, Loss: 48.40959407068403\n",
      "Epoch 6/10, Loss: 47.42725778925321\n",
      "Epoch 7/10, Loss: 47.38398640158736\n",
      "Epoch 8/10, Loss: 46.844275036689695\n",
      "Epoch 9/10, Loss: 46.37939709221959\n",
      "Epoch 10/10, Loss: 45.82382053266754\n",
      "Fold 1 AUCs: {'NR.AhR': 0.9998168498168498, 'NR.AR': 0.9129129129129129, 'NR.AR.LBD': 0.8691394658753709, 'NR.Aromatase': 0.7383300460223536, 'NR.ER': 0.7600979192166463, 'NR.ER.LBD': 0.9306164183464796, 'NR.PPAR.gamma': 0.8042016806722688, 'SR.ARE': 0.7404761904761905, 'SR.ATAD5': 0.8012436665131276, 'SR.HSE': 0.6270029673590505, 'SR.MMP': 0.8214285714285714, 'SR.p53': 0.791566265060241}\n",
      "Epoch 1/10, Loss: 60.60493838137437\n",
      "Epoch 2/10, Loss: 52.124519833335675\n",
      "Epoch 3/10, Loss: 50.38657640647499\n",
      "Epoch 4/10, Loss: 49.222499958851074\n",
      "Epoch 5/10, Loss: 47.90199815106327\n",
      "Epoch 6/10, Loss: 47.32719528423343\n",
      "Epoch 7/10, Loss: 46.94420860037817\n",
      "Epoch 8/10, Loss: 46.89870560916308\n",
      "Epoch 9/10, Loss: 46.53023852598765\n",
      "Epoch 10/10, Loss: 46.065510988885116\n",
      "Fold 2 AUCs: {'NR.AhR': 0.9983029653447659, 'NR.AR': 0.8465909090909092, 'NR.AR.LBD': 0.8546218487394958, 'NR.Aromatase': 0.8760932944606413, 'NR.ER': 0.6945970695970696, 'NR.ER.LBD': 0.8890547263681593, 'NR.PPAR.gamma': 0.7296511627906976, 'SR.ARE': 0.7296908698777858, 'SR.ATAD5': 0.9711632453567938, 'SR.HSE': 0.7023460410557185, 'SR.MMP': 0.8137412775093935, 'SR.p53': 0.7717717717717718}\n",
      "Epoch 1/10, Loss: 60.8587672327942\n",
      "Epoch 2/10, Loss: 51.96615669707836\n",
      "Epoch 3/10, Loss: 50.42825474596153\n",
      "Epoch 4/10, Loss: 49.431093805817234\n",
      "Epoch 5/10, Loss: 48.42280493380263\n",
      "Epoch 6/10, Loss: 47.82481547743488\n",
      "Epoch 7/10, Loss: 46.76146492508192\n",
      "Epoch 8/10, Loss: 47.034238007153085\n",
      "Epoch 9/10, Loss: 46.142953845070885\n",
      "Epoch 10/10, Loss: 46.344866955637606\n",
      "Fold 3 AUCs: {'NR.AhR': 1.0, 'NR.AR': 0.7872455902306649, 'NR.AR.LBD': 0.9426885798567214, 'NR.Aromatase': 0.8950695322376738, 'NR.ER': 0.68915770609319, 'NR.ER.LBD': 0.8816824966078698, 'NR.PPAR.gamma': 0.7876832844574779, 'SR.ARE': 0.7906652734238941, 'SR.ATAD5': 0.6913690476190476, 'SR.HSE': 0.718063872255489, 'SR.MMP': 0.8250224618149147, 'SR.p53': 0.7589285714285714}\n",
      "Epoch 1/10, Loss: 60.52826124575872\n",
      "Epoch 2/10, Loss: 52.115618342965114\n",
      "Epoch 3/10, Loss: 50.93480891817597\n",
      "Epoch 4/10, Loss: 49.41816619436487\n",
      "Epoch 5/10, Loss: 48.18556119245794\n",
      "Epoch 6/10, Loss: 47.74448112055294\n",
      "Epoch 7/10, Loss: 47.32948177971697\n",
      "Epoch 8/10, Loss: 47.2708097334129\n",
      "Epoch 9/10, Loss: 46.14490019008314\n",
      "Epoch 10/10, Loss: 46.12760521110137\n",
      "Fold 4 AUCs: {'NR.AhR': 0.9989550679205851, 'NR.AR': 0.816198224852071, 'NR.AR.LBD': 0.812202380952381, 'NR.Aromatase': 0.7904827904827905, 'NR.ER': 0.6760210473565522, 'NR.ER.LBD': 0.8706948640483383, 'NR.PPAR.gamma': 0.9560117302052785, 'SR.ARE': 0.7963836477987422, 'SR.ATAD5': 0.8583333333333333, 'SR.HSE': 0.8187947745469869, 'SR.MMP': 0.8715167197452229, 'SR.p53': 0.882843137254902}\n",
      "Epoch 1/10, Loss: 59.81614591380231\n",
      "Epoch 2/10, Loss: 51.95670541897619\n",
      "Epoch 3/10, Loss: 49.581093246183215\n",
      "Epoch 4/10, Loss: 48.669917630869136\n",
      "Epoch 5/10, Loss: 48.41589683830251\n",
      "Epoch 6/10, Loss: 47.788563174472195\n",
      "Epoch 7/10, Loss: 46.91312398052995\n",
      "Epoch 8/10, Loss: 46.64677467495609\n",
      "Epoch 9/10, Loss: 46.44436972147762\n",
      "Epoch 10/10, Loss: 45.27227074866074\n",
      "Fold 5 AUCs: {'NR.AhR': 0.9991203518592563, 'NR.AR': 0.7610119047619048, 'NR.AR.LBD': 0.8047619047619048, 'NR.Aromatase': 0.9345238095238095, 'NR.ER': 0.7523923444976076, 'NR.ER.LBD': 0.7810674723061429, 'NR.PPAR.gamma': 0.9172619047619047, 'SR.ARE': 0.7172057502246182, 'SR.ATAD5': 0.789096789096789, 'SR.HSE': 0.7698648203099242, 'SR.MMP': 0.8352425876010782, 'SR.p53': 0.8685636856368564}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "cv_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(df_cv_filtered)):\n",
    "    fold_train_set = df_cv_filtered.iloc[train_idx]\n",
    "    fold_val_set = df_cv_filtered.iloc[val_idx]\n",
    "\n",
    "    # combine fold train set with final train set\n",
    "    combined_train_set = pd.concat([df_final_train, fold_train_set], axis=0)\n",
    "\n",
    "    # separate features and labels\n",
    "    X_train = combined_train_set.iloc[:, 1:-12]\n",
    "    y_train = combined_train_set.iloc[:, -13:]\n",
    "    X_val = fold_val_set.iloc[:, 1:-12]\n",
    "    y_val = fold_val_set.iloc[:, -13:]\n",
    "\n",
    "    # Impute feature vectors via median imputation\n",
    "    X_train = imputer.fit_transform(X_train)\n",
    "    X_val = imputer.transform(X_val)\n",
    "\n",
    "    # standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train) # this converts to numpy array\n",
    "    X_val = scaler.transform(X_val) # this converts to numpy array\n",
    "\n",
    "    # Drop last column of y_train and y_val which is the label_density\n",
    "    y_train = y_train.iloc[:, :-1]\n",
    "    y_val = y_val.iloc[:, :-1].fillna(y_val.median())\n",
    "\n",
    "    # Reset indices for alignment\n",
    "    y_val.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # initialize model\n",
    "    model = MultiTaskToxicityModel(input_dim=X_train.shape[1])\n",
    "\n",
    "    # initialize optimizer\n",
    "    optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # initialize loss function\n",
    "    loss_fn = nn.BCELoss(reduction='none')\n",
    "\n",
    "    # train model\n",
    "    model = train_model(model, optimizer, X_train, y_train, epochs, batch_size)\n",
    "\n",
    "    fold_results = {}\n",
    "    \n",
    "    for task_idx, task in enumerate(y_train.columns):  # Use task index for slicing y_pred_task\n",
    "        # Get the target values for the current task\n",
    "        y_val_task = y_val[task]\n",
    "\n",
    "        # Align features (X_val) with the corresponding indices in y_val_task\n",
    "        X_val_task = X_val[y_val_task.index.to_numpy(), :]  # Use NumPy-style indexing\n",
    "\n",
    "        # Predict using the model\n",
    "        y_pred_task = model(torch.tensor(X_val_task, dtype=torch.float32)).detach().numpy()\n",
    "\n",
    "        # Extract predictions for the current task (task_idx)\n",
    "        y_pred_task_specific = y_pred_task[:, task_idx]  # Select the column corresponding to the task\n",
    "\n",
    "        # Compute AUC for the current task\n",
    "        auc = roc_auc_score(y_val_task, y_pred_task_specific)\n",
    "        fold_results[task] = auc\n",
    "\n",
    "    \n",
    "    # Store results for this fold\n",
    "    cv_results.append(fold_results)\n",
    "    print(f\"Fold {fold + 1} AUCs: {fold_results}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean AUCs for each task: NR.AhR           0.999239\n",
      "NR.AR            0.824792\n",
      "NR.AR.LBD        0.856683\n",
      "NR.Aromatase     0.846900\n",
      "NR.ER            0.714453\n",
      "NR.ER.LBD        0.870623\n",
      "NR.PPAR.gamma    0.838962\n",
      "SR.ARE           0.754884\n",
      "SR.ATAD5         0.822241\n",
      "SR.HSE           0.727214\n",
      "SR.MMP           0.833390\n",
      "SR.p53           0.814735\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# convert results to a DataFrame for easier analysis\n",
    "cv_results_df = pd.DataFrame(cv_results)\n",
    "\n",
    "# calculate mean auc for each task across all folds\n",
    "mean_aucs = cv_results_df.mean(axis=0)\n",
    "\n",
    "print('Mean AUCs for each task:', mean_aucs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epochs = 10\n",
    "best_batch_size = 32\n",
    "best_learning_rate = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Training Round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10328, 802)\n",
      "(10328, 12)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 199.1561598910636\n",
      "Epoch 2/10, Loss: 199.2553668125495\n",
      "Epoch 3/10, Loss: 199.25536887343085\n",
      "Epoch 4/10, Loss: 199.25536785775293\n",
      "Epoch 5/10, Loss: 199.2553679965229\n",
      "Epoch 6/10, Loss: 199.25536783154905\n",
      "Epoch 7/10, Loss: 199.25536805852647\n",
      "Epoch 8/10, Loss: 199.25536815300813\n",
      "Epoch 9/10, Loss: 199.2553675270671\n",
      "Epoch 10/10, Loss: 199.25536843645315\n"
     ]
    }
   ],
   "source": [
    "# Combine the entire dataset for final training\n",
    "X_final_train = df_final_train.iloc[:, 1:-12]\n",
    "y_final_train = df_final_train.iloc[:, -12:]\n",
    "\n",
    "print(X_final_train.shape)\n",
    "print(y_final_train.shape)\n",
    "\n",
    "model = MultiTaskToxicityModel(input_dim=X_final_train.shape[1])\n",
    "optimizer = Adam(model.parameters(), lr=best_learning_rate)\n",
    "\n",
    "# scale features\n",
    "X_final_train = scaler.fit_transform(X_final_train)\n",
    "\n",
    "# impute missing values\n",
    "X_final_train = imputer.fit_transform(X_final_train)\n",
    "\n",
    "# Train final model with optimized parameters\n",
    "final_model = train_model(model, optimizer, X_final_train, y_final_train, best_epochs, best_batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
