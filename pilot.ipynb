{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPLORATORY DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 21:45:10.262209: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-09 21:45:10.536456: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-09 21:45:10.636244: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-09 21:45:10.661112: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-09 21:45:10.837200: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-09 21:45:13.522681: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1731217553.933120    3860 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-09 21:45:54.086104: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2343] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qamil/.pyenv/versions/3.11.9/lib/python3.11/site-packages/keras/src/optimizers/base_optimizer.py:731: UserWarning: Gradients do not exist for variables ['kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - loss: 1.4319\n",
      "Epoch 2/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.8190\n",
      "Epoch 3/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.8462\n",
      "Epoch 4/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.8108\n",
      "Epoch 5/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.8181\n",
      "Epoch 6/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.8526\n",
      "Epoch 7/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.8394\n",
      "Epoch 8/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.8433\n",
      "Epoch 9/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.8040\n",
      "Epoch 10/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.8437\n",
      "Epoch 11/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.8162\n",
      "Epoch 12/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.8162\n",
      "Epoch 13/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.8325\n",
      "Epoch 14/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.8408\n",
      "Epoch 15/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.8032\n",
      "Epoch 16/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.8315\n",
      "Epoch 17/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.8451\n",
      "Epoch 18/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.8186\n",
      "Epoch 19/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.8091\n",
      "Epoch 20/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-0s\u001b[0m -987us/step - loss: 0.8218\n",
      "Epoch 21/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.8404\n",
      "Epoch 22/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.8282\n",
      "Epoch 23/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.8084\n",
      "Epoch 24/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.8262\n",
      "Epoch 25/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.8403\n",
      "Epoch 26/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.8276\n",
      "Epoch 27/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.8195\n",
      "Epoch 28/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.8216\n",
      "Epoch 29/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.8150\n",
      "Epoch 30/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.8145\n",
      "Epoch 31/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.8493\n",
      "Epoch 32/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.8165\n",
      "Epoch 33/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.8249\n",
      "Epoch 34/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.8202\n",
      "Epoch 35/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.8160\n",
      "Epoch 36/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 944us/step - loss: 0.8372\n",
      "Epoch 37/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.8188\n",
      "Epoch 38/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.8317\n",
      "Epoch 39/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.8290\n",
      "Epoch 40/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.8114\n",
      "Epoch 41/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.8160\n",
      "Epoch 42/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.8377\n",
      "Epoch 43/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.8197\n",
      "Epoch 44/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.8220\n",
      "Epoch 45/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.8200\n",
      "Epoch 46/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.8363\n",
      "Epoch 47/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.7981\n",
      "Epoch 48/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.8228\n",
      "Epoch 49/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.8250\n",
      "Epoch 50/50\n",
      "\u001b[1m189/189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.8380\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load your data (replace these with actual paths or data loading methods)\n",
    "x_train = pd.read_csv('data/tox21_dense_train.csv')\n",
    "y_train = pd.read_csv('data/tox21_labels_train.csv')\n",
    "\n",
    "# Preprocessing\n",
    "compound_ids = x_train['Unnamed: 0']\n",
    "x_train = x_train.drop(columns=['Unnamed: 0'])\n",
    "y_train = y_train.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# Create masks based on valid labels in y_train\n",
    "y_train_mask = ~y_train.isna()\n",
    "y_train = y_train.fillna(0)  # Replace NaNs with zeros for computation\n",
    "\n",
    "# Model Parameters\n",
    "num_features = x_train.shape[1]\n",
    "num_tasks = y_train.shape[1]\n",
    "num_hidden_units = 1024  # Adjust based on resources\n",
    "dropout_rate = 0.5\n",
    "learning_rate = 0.001\n",
    "\n",
    "# TensorFlow Model\n",
    "class ToxicityPredictionModel(tf.keras.Model):\n",
    "    def __init__(self, num_features, num_tasks, num_hidden_units, dropout_rate):\n",
    "        super(ToxicityPredictionModel, self).__init__()\n",
    "        self.hidden_layer = tf.keras.layers.Dense(num_hidden_units, activation='relu')\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.output_layers = [tf.keras.layers.Dense(1, activation='sigmoid') for _ in range(num_tasks)]\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.hidden_layer(inputs)\n",
    "        x = self.dropout(x, training=training)\n",
    "        return [layer(x) for layer in self.output_layers]\n",
    "\n",
    "# Instantiate the model\n",
    "model = ToxicityPredictionModel(num_features, num_tasks, num_hidden_units, dropout_rate)\n",
    "\n",
    "# Custom Loss Function with Masked Weighted Binary Cross-Entropy\n",
    "def masked_loss(y_true, y_pred, mask):\n",
    "    loss = 0\n",
    "    for task_idx in range(num_tasks):\n",
    "        y_t = y_true[:, task_idx]\n",
    "        y_p = y_pred[task_idx]\n",
    "        m = tf.cast(mask[:, task_idx], tf.float32)\n",
    "        \n",
    "        # Binary cross-entropy for each task with masking\n",
    "        bce = tf.keras.losses.binary_crossentropy(y_t, y_p)\n",
    "        masked_bce = tf.multiply(bce, m)\n",
    "        \n",
    "        # Accumulate weighted loss for each task\n",
    "        task_loss = tf.reduce_sum(masked_bce) / tf.reduce_sum(m + 1e-10)\n",
    "        loss += task_loss\n",
    "    return loss / num_tasks\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "              loss=lambda y_true, y_pred: masked_loss(y_true, y_pred, y_train_mask.values))\n",
    "\n",
    "# Training the model\n",
    "x_train_np = x_train.values\n",
    "y_train_np = y_train.values\n",
    "\n",
    "# Fit model\n",
    "history = model.fit(x_train_np, y_train_np, epochs=50, batch_size=64, verbose=1)\n",
    "\n",
    "# Model Evaluation\n",
    "# Use similar masked loss or AUC for evaluation if applicable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1cAAAIjCAYAAADvBuGTAAAAP3RFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMS5wb3N0MSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8kixA/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAB1E0lEQVR4nO3deVxU9f7H8fewzaACboiKuJv7rpDaeuWXaXnV6y1TS6XSFq3M2y0tt+wmrV5KTc3SzLRsUdvUMtquZS6gprnnGgpoKijIOuf3B84xEjcEzsz0ej4e87hy5syZ78Bc4j2f7/fztRmGYQgAAAAAcEV8rB4AAAAAAHgDwhUAAAAAlADCFQAAAACUAMIVAAAAAJQAwhUAAAAAlADCFQAAAACUAMIVAAAAAJQAwhUAAAAAlADCFQAAAACUAMIVAHipIUOGqG7dusV67MSJE2Wz2Up2QEAR3nrrLdlsNq1fv97qoQDAFSNcAUAZs9lsl3T79ttvrR6qJYYMGaIKFSpYPQyv4Qov57v99NNPVg8RALyGn9UDAIC/mvnz5xf6+u2339bKlSvPOd60adMrep7Zs2fL6XQW67Fjx47V6NGjr+j54V4mTZqkevXqnXO8YcOGFowGALwT4QoAytidd95Z6OuffvpJK1euPOf4n2VmZqpcuXKX/Dz+/v7FGp8k+fn5yc+P/0R4ioyMDJUvX/6C53Tv3l0dOnQooxEBwF8T0wIBwA3dcMMNatGihRISEnTdddepXLlyevLJJyVJH3/8sW655RbVrFlTdrtdDRo00DPPPKP8/PxC1/jzmqt9+/bJZrPppZde0uuvv64GDRrIbrerY8eOWrduXaHHFrXmymazacSIEVq6dKlatGghu92u5s2ba8WKFeeM/9tvv1WHDh3kcDjUoEEDzZo1q8TXcX3wwQdq3769AgMDVbVqVd15551KSkoqdE5ycrJiYmJUq1Yt2e121ahRQ7169dK+ffvMc9avX69u3bqpatWqCgwMVL169XT33Xdf0hhee+01NW/eXHa7XTVr1tTw4cN14sQJ8/4RI0aoQoUKyszMPOex/fv3V/Xq1Qv93JYvX65rr71W5cuXV1BQkG655Rb98ssvhR7nmjb566+/qkePHgoKCtLAgQMvabwX8sf3x3//+1/VqVNHgYGBuv7667Vly5Zzzv/666/NsVasWFG9evXStm3bzjkvKSlJ99xzj/l+rVevnh544AHl5OQUOi87O1ujRo1SaGioypcvrz59+ujIkSOFzrmSnxUAlAU+lgQAN/X777+re/fuuuOOO3TnnXcqLCxMUsEamgoVKmjUqFGqUKGCvv76a40fP17p6el68cUXL3rdhQsX6uTJk7rvvvtks9n0wgsv6B//+If27Nlz0WrXqlWrtHjxYj344IMKCgrSq6++qr59++rAgQOqUqWKJGnDhg26+eabVaNGDT399NPKz8/XpEmTFBoaeuXflDPeeustxcTEqGPHjoqNjVVKSopeeeUV/fDDD9qwYYMqVqwoSerbt69++eUXPfTQQ6pbt65SU1O1cuVKHThwwPz6pptuUmhoqEaPHq2KFStq3759Wrx48UXHMHHiRD399NOKjo7WAw88oB07dmjGjBlat26dfvjhB/n7+6tfv36aPn26Pv/8c912223mYzMzM/Xpp59qyJAh8vX1lVQwXXTw4MHq1q2bnn/+eWVmZmrGjBm65pprtGHDhkJBOS8vT926ddM111yjl1566ZIqmmlpaTp69GihYzabzfy5ubz99ts6efKkhg8frqysLL3yyiv629/+ps2bN5vvwa+++krdu3dX/fr1NXHiRJ0+fVpTp05Vly5dlJiYaI710KFDioyM1IkTJzRs2DA1adJESUlJ+vDDD5WZmamAgADzeR966CFVqlRJEyZM0L59+xQXF6cRI0Zo0aJFknRFPysAKDMGAMBSw4cPN/786/j66683JBkzZ8485/zMzMxzjt13331GuXLljKysLPPY4MGDjTp16phf792715BkVKlSxTh27Jh5/OOPPzYkGZ9++ql5bMKECeeMSZIREBBg7N692zy2adMmQ5IxdepU81jPnj2NcuXKGUlJSeaxXbt2GX5+fudcsyiDBw82ypcvf977c3JyjGrVqhktWrQwTp8+bR7/7LPPDEnG+PHjDcMwjOPHjxuSjBdffPG811qyZIkhyVi3bt1Fx/VHqampRkBAgHHTTTcZ+fn55vFp06YZkow5c+YYhmEYTqfTCA8PN/r27Vvo8e+//74hyfj+++8NwzCMkydPGhUrVjSGDh1a6Lzk5GQjJCSk0PHBgwcbkozRo0df0ljnzp1rSCryZrfbzfNc74/AwEDjt99+M4+vWbPGkGQ8+uij5rE2bdoY1apVM37//Xfz2KZNmwwfHx9j0KBB5rFBgwYZPj4+RX5/nU5nofFFR0ebxwzDMB599FHD19fXOHHihGEYxf9ZAUBZYlogALgpu92umJiYc44HBgaa/z558qSOHj2qa6+9VpmZmdq+fftFr9uvXz9VqlTJ/Praa6+VJO3Zs+eij42OjlaDBg3Mr1u1aqXg4GDzsfn5+frqq6/Uu3dv1axZ0zyvYcOG6t69+0WvfynWr1+v1NRUPfjgg3I4HObxW265RU2aNNHnn38uqeD7FBAQoG+//VbHjx8v8lquCtdnn32m3NzcSx7DV199pZycHI0cOVI+Pmf/Uzp06FAFBwebY7DZbLrtttu0bNkynTp1yjxv0aJFCg8P1zXXXCNJWrlypU6cOKH+/fvr6NGj5s3X11dRUVH65ptvzhnDAw88cMnjlaTp06dr5cqVhW7Lly8/57zevXsrPDzc/DoyMlJRUVFatmyZJOnw4cPauHGjhgwZosqVK5vntWrVSv/3f/9nnud0OrV06VL17NmzyLVef54iOmzYsELHrr32WuXn52v//v2Siv+zAoCyRLgCADcVHh5eaNqUyy+//KI+ffooJCREwcHBCg0NNZthpKWlXfS6tWvXLvS1K2idL4Bc6LGux7sem5qaqtOnTxfZga6kutK5/thu3LjxOfc1adLEvN9ut+v555/X8uXLFRYWpuuuu04vvPCCkpOTzfOvv/569e3bV08//bSqVq2qXr16ae7cucrOzi7WGAICAlS/fn3zfqkgzJ4+fVqffPKJJOnUqVNatmyZbrvtNjNM7Nq1S5L0t7/9TaGhoYVuX375pVJTUws9j5+fn2rVqnXxb9YfREZGKjo6utDtxhtvPOe8Ro0anXPsqquuMtepXej737RpUx09elQZGRk6cuSI0tPT1aJFi0sa38Xel8X9WQFAWSJcAYCb+mOFyuXEiRO6/vrrtWnTJk2aNEmffvqpVq5cqeeff16SLqn1umuNz58ZhlGqj7XCyJEjtXPnTsXGxsrhcGjcuHFq2rSpNmzYIKmgevLhhx9q9erVGjFihJKSknT33Xerffv2hSpNV+Lqq69W3bp19f7770uSPv30U50+fVr9+vUzz3H93ObPn39OdWnlypX6+OOPC13TbrcXqph5g4u9t8riZwUAV8q7fjMDgJf79ttv9fvvv+utt97SI488oltvvVXR0dGFpvlZqVq1anI4HNq9e/c59xV1rDjq1KkjSdqxY8c59+3YscO836VBgwb617/+pS+//FJbtmxRTk6OXn755ULnXH311Xr22We1fv16LViwQL/88ovee++9yx5DTk6O9u7de84Ybr/9dq1YsULp6elatGiR6tatq6uvvrrQGKWC79+fq0vR0dG64YYbLvJdKTmuKtof7dy502xScaHv//bt21W1alWVL19eoaGhCg4OLrLT4JW43J8VAJQlwhUAeBDXp/t/rBTl5OTotddes2pIhfj6+io6OlpLly7VoUOHzOO7d+8ucn1PcXTo0EHVqlXTzJkzC00JW758ubZt26ZbbrlFUkFHvqysrEKPbdCggYKCgszHHT9+/JyqW5s2bSTpgtPNoqOjFRAQoFdffbXQ4998802lpaWZY3Dp16+fsrOzNW/ePK1YsUK33357ofu7deum4OBgTZ48ucj1RH9uSV6ali5dWqil/dq1a7VmzRpzzVyNGjXUpk0bzZs3r1Db+S1btujLL79Ujx49JEk+Pj7q3bu3Pv30U61fv/6c57ncamdxf1YAUJZoxQ4AHqRz586qVKmSBg8erIcfflg2m03z5893q2l5EydO1JdffqkuXbrogQceUH5+vqZNm6YWLVpo48aNl3SN3Nxc/ec//znneOXKlfXggw/q+eefV0xMjK6//nr179/fbMVet25dPfroo5IKqi1du3bV7bffrmbNmsnPz09LlixRSkqK7rjjDknSvHnz9Nprr6lPnz5q0KCBTp48qdmzZys4ONgMCUUJDQ3VmDFj9PTTT+vmm2/W3//+d+3YsUOvvfaaOnbseM6G0O3atVPDhg311FNPKTs7u9CUQEkKDg7WjBkzdNddd6ldu3a64447FBoaqgMHDujzzz9Xly5dNG3atEv63p3P8uXLi2x40rlzZ9WvX9/8umHDhrrmmmv0wAMPKDs7W3FxcapSpYoef/xx85wXX3xR3bt3V6dOnXTPPfeYrdhDQkI0ceJE87zJkyfryy+/1PXXX69hw4apadOmOnz4sD744AOtWrXKbFJxKYr7swKAskS4AgAPUqVKFX322Wf617/+pbFjx6pSpUq688471bVrV3Xr1s3q4UmS2rdvr+XLl+uxxx7TuHHjFBERoUmTJmnbtm2X1M1QKqjGjRs37pzjDRo00IMPPqghQ4aoXLlyeu655/TEE0+Ym84+//zz5h/sERER6t+/v+Lj4zV//nz5+fmpSZMmev/999W3b19JBU0S1q5dq/fee08pKSkKCQlRZGSkFixYoHr16l1wjBMnTlRoaKimTZumRx99VJUrV9awYcM0efLkIvcL69evn5599lk1bNhQ7dq1O+f+AQMGqGbNmnruuef04osvKjs7W+Hh4br22muL7Bp5ucaPH1/k8blz5xYKV4MGDZKPj4/i4uKUmpqqyMhITZs2TTVq1DDPiY6O1ooVKzRhwgSNHz9e/v7+uv766/X8888X+r6Fh4drzZo1GjdunBYsWKD09HSFh4ere/ful7Q31x9dyc8KAMqKzXCnjzsBAF6rd+/e+uWXX4pc0wPr7du3T/Xq1dOLL76oxx57zOrhAIBHYs0VAKDEnT59utDXu3bt0rJly8q0MQMAAGWNaYEAgBJXv359DRkyxNzzacaMGQoICCi0bgcAAG9DuAIAlLibb75Z7777rpKTk2W329WpUydNnjy5yA1qAQDwFqy5AgAAAIASwJorAAAAACgBhCsAAAAAKAGsuSqC0+nUoUOHFBQUJJvNZvVwAAAAAFjEMAydPHlSNWvWlI/PhWtThKsiHDp0SBEREVYPAwAAAICbOHjwoGrVqnXBcwhXRQgKCpJU8A0MDg62eDQAAAAArJKenq6IiAgzI1wI4aoIrqmAwcHBhCsAAAAAl7RciIYWAAAAAFACCFcAAAAAUAIIVwAAAABQAlhzBQAAAK+Wn5+v3Nxcq4cBN+Xr6ys/P78S2YKJcAUAAACvderUKf32228yDMPqocCNlStXTjVq1FBAQMAVXYdwBQAAAK+Un5+v3377TeXKlVNoaGiJVCbgXQzDUE5Ojo4cOaK9e/eqUaNGF90o+EIIVwAAAPBKubm5MgxDoaGhCgwMtHo4cFOBgYHy9/fX/v37lZOTI4fDUexr0dACAAAAXo2KFS7mSqpVha5TIlcBAAAAgL84whUAAAAAlADCFQAAAODl6tatq7i4uEs+/9tvv5XNZtOJEydKbUzeiHAFAAAAuAmbzXbB28SJE4t13XXr1mnYsGGXfH7nzp11+PBhhYSEFOv5LpW3hTi6BQIAAABu4vDhw+a/Fy1apPHjx2vHjh3msQoVKpj/NgxD+fn58vO7+J/0oaGhlzWOgIAAVa9e/bIeAypXAAAA+IswDEOZOXmW3C51E+Pq1aubt5CQENlsNvPr7du3KygoSMuXL1f79u1lt9u1atUq/frrr+rVq5fCwsJUoUIFdezYUV999VWh6/55WqDNZtMbb7yhPn36qFy5cmrUqJE++eQT8/4/V5TeeustVaxYUV988YWaNm2qChUq6Oabby4UBvPy8vTwww+rYsWKqlKlip544gkNHjxYvXv3LvbP7Pjx4xo0aJAqVaqkcuXKqXv37tq1a5d5//79+9WzZ09VqlRJ5cuXV/PmzbVs2TLzsQMHDjRb8Tdq1Ehz584t9lguBZUrAAAA/CWczs1Xs/FfWPLcWyd1U7mAkvnTe/To0XrppZdUv359VapUSQcPHlSPHj307LPPym636+2331bPnj21Y8cO1a5d+7zXefrpp/XCCy/oxRdf1NSpUzVw4EDt379flStXLvL8zMxMvfTSS5o/f758fHx055136rHHHtOCBQskSc8//7wWLFiguXPnqmnTpnrllVe0dOlS3XjjjcV+rUOGDNGuXbv0ySefKDg4WE888YR69OihrVu3yt/fX8OHD1dOTo6+//57lS9fXlu3bjWre+PGjdPWrVu1fPlyVa1aVbt379bp06eLPZZLQbgCAAAAPMikSZP0f//3f+bXlStXVuvWrc2vn3nmGS1ZskSffPKJRowYcd7rDBkyRP3795ckTZ48Wa+++qrWrl2rm2++ucjzc3NzNXPmTDVo0ECSNGLECE2aNMm8f+rUqRozZoz69OkjSZo2bZpZRSoOV6j64Ycf1LlzZ0nSggULFBERoaVLl+q2227TgQMH1LdvX7Vs2VKSVL9+ffPxBw4cUNu2bdWhQwdJBdW70ka4cnObDp5Q0onTalYjWHWrlrd6OAAAAB4r0N9XWyd1s+y5S4orLLicOnVKEydO1Oeff67Dhw8rLy9Pp0+f1oEDBy54nVatWpn/Ll++vIKDg5Wamnre88uVK2cGK0mqUaOGeX5aWppSUlIUGRlp3u/r66v27dvL6XRe1utz2bZtm/z8/BQVFWUeq1Kliho3bqxt27ZJkh5++GE98MAD+vLLLxUdHa2+ffuar+uBBx5Q3759lZiYqJtuukm9e/c2Q1ppYc2Vm5v53a96cEGi/rfriNVDAQAA8Gg2m03lAvwsudlsthJ7HeXLF/7A/bHHHtOSJUs0efJk/e9//9PGjRvVsmVL5eTkXPA6/v7+53x/LhSEijr/UteSlZZ7771Xe/bs0V133aXNmzerQ4cOmjp1qiSpe/fu2r9/vx599FEdOnRIXbt21WOPPVaq4yFcuTnHmU85snKLl/gBAADg3X744QcNGTJEffr0UcuWLVW9enXt27evTMcQEhKisLAwrVu3zjyWn5+vxMTEYl+zadOmysvL05o1a8xjv//+u3bs2KFmzZqZxyIiInT//fdr8eLF+te//qXZs2eb94WGhmrw4MF65513FBcXp9dff73Y47kUTAt0c3a/gvyblZtv8UgAAADgjho1aqTFixerZ8+estlsGjduXLGn4l2Jhx56SLGxsWrYsKGaNGmiqVOn6vjx45dUtdu8ebOCgoLMr202m1q3bq1evXpp6NChmjVrloKCgjR69GiFh4erV69ekqSRI0eqe/fuuuqqq3T8+HF98803atq0qSRp/Pjxat++vZo3b67s7Gx99tln5n2lhXDl5szKVR7hCgAAAOeaMmWK7r77bnXu3FlVq1bVE088ofT09DIfxxNPPKHk5GQNGjRIvr6+GjZsmLp16yZf34uvN7vuuusKfe3r66u8vDzNnTtXjzzyiG699Vbl5OTouuuu07Jly8wpivn5+Ro+fLh+++03BQcH6+abb9Z///tfSQV7dY0ZM0b79u1TYGCgrr32Wr333nsl/8L/wGZYPVHSDaWnpyskJERpaWkKDg62dCyxy7dp1nd7dO819TT21mYXfwAAAAAkSVlZWdq7d6/q1asnh8Nh9XD+cpxOp5o2barbb79dzzzzjNXDuaALvVcuJxtQuXJzDj8qVwAAAHB/+/fv15dffqnrr79e2dnZmjZtmvbu3asBAwZYPbQyQ0MLN0dDCwAAAHgCHx8fvfXWW+rYsaO6dOmizZs366uvvir1dU7uhMqVm6OhBQAAADxBRESEfvjhB6uHYSkqV27OVbnKzqNyBQAAALgzy8PV9OnTVbduXTkcDkVFRWnt2rXnPTc3N1eTJk1SgwYN5HA41Lp1a61YsaLQOfn5+Ro3bpzq1aunwMBANWjQQM8884zlG5wVl8OfyhUAAMCV8NS/A1F2Suo9Ymm4WrRokUaNGqUJEyYoMTFRrVu3Vrdu3ZSamlrk+WPHjtWsWbM0depUbd26Vffff7/69OmjDRs2mOc8//zzmjFjhqZNm6Zt27bp+eef1wsvvGDu1OxpzMoVa64AAAAui6sFeE5OjsUjgbvLzMyUJLPFe3FZ2oo9KipKHTt21LRp0yQVtGuMiIjQQw89pNGjR59zfs2aNfXUU09p+PDh5rG+ffsqMDBQ77zzjiTp1ltvVVhYmN58883znnMx7tSKPX5biu6Zt16taoXokxHXWDoWAAAAT2IYhg4cOKDc3FzVrFlTPj6WT9qCmzEMQ5mZmUpNTVXFihVVo0aNc87xiFbsOTk5SkhI0JgxY8xjPj4+io6O1urVq4t8THZ29jl95wMDA7Vq1Srz686dO+v111/Xzp07ddVVV2nTpk1atWqVpkyZct6xZGdnKzs72/zaik3Xzudst0CmBQIAAFwOm82mGjVqaO/evdq/f7/Vw4Ebq1ixoqpXr37F17EsXB09elT5+fkKCwsrdDwsLEzbt28v8jHdunXTlClTdN1116lBgwaKj4/X4sWLlZ9/NniMHj1a6enpatKkiXx9fZWfn69nn31WAwcOPO9YYmNj9fTTT5fMCythrjVXNLQAAAC4fAEBAWrUqBFTA3Fe/v7+5hTSK+VRrdhfeeUVDR06VE2aNJHNZlODBg0UExOjOXPmmOe8//77WrBggRYuXKjmzZtr48aNGjlypGrWrKnBgwcXed0xY8Zo1KhR5tfp6emKiIgo9ddzKex+VK4AAACuhI+Pzzmzn4DSYFm4qlq1qnx9fZWSklLoeEpKynlLcqGhoVq6dKmysrL0+++/q2bNmho9erTq169vnvPvf/9bo0eP1h133CFJatmypfbv36/Y2Njzhiu73S673V5Cr6xkne0WSOUKAAAAcGeWreoLCAhQ+/btFR8fbx5zOp2Kj49Xp06dLvhYh8Oh8PBw5eXl6aOPPlKvXr3M+zIzM89ZrOjr6yun0zPDCZUrAAAAwDNYOi1w1KhRGjx4sDp06KDIyEjFxcUpIyNDMTExkqRBgwYpPDxcsbGxkqQ1a9YoKSlJbdq0UVJSkiZOnCin06nHH3/cvGbPnj317LPPqnbt2mrevLk2bNigKVOm6O6777bkNV6pP24ibBiGbDabxSMCAAAAUBRLw1W/fv105MgRjR8/XsnJyWrTpo1WrFhhNrk4cOBAoSpUVlaWxo4dqz179qhChQrq0aOH5s+fr4oVK5rnTJ06VePGjdODDz6o1NRU1axZU/fdd5/Gjx9f1i+vRLimBUoFAcsVtgAAAAC4F0v3uXJX7rTPVU6eU1eNXS5J2jT+JoWUu7KNzQAAAABcusvJBuyk5ub8fW3yOTMTMCuPdVcAAACAuyJcuTmbzXZ23RUdAwEAAAC3RbjyAK5wReUKAAAAcF+EKw/g8HPtdUW4AgAAANwV4coD2F2VK6YFAgAAAG6LcOUB7GcqV9lMCwQAAADcFuHKAzioXAEAAABuj3DlAVwbCbPmCgAAAHBfhCsPYPdzVa4IVwAAAIC7Ilx5ALNylce0QAAAAMBdEa48wNlNhKlcAQAAAO6KcOUBHGemBWZTuQIAAADcFuHKA9hpaAEAAAC4PcKVBzjbip1wBQAAALgrwpUHcJibCDMtEAAAAHBXhCsPYKdyBQAAALg9wpUHsPu51lxRuQIAAADcFeHKA7DmCgAAAHB/hCsPYO5zxZorAAAAwG0RrjyAg1bsAAAAgNsjXHkA1ybCWVSuAAAAALdFuPIArk2Es6lcAQAAAG6LcOUBWHMFAAAAuD/ClQcwpwVSuQIAAADcFuHKA9DQAgAAAHB/hCsPYDcrV0wLBAAAANwV4coDmJWrvHwZhmHxaAAAAAAUhXDlAexnGloYhpSbT7gCAAAA3BHhygO4KldSQfUKAAAAgPshXHmAAF8f2WwF/6apBQAAAOCeCFcewGazye7n2kiYphYAAACAOyJceYizGwlTuQIAAADcEeHKQzhoxw4AAAC4NcKVh7CzkTAAAADg1ghXHoLKFQAAAODeCFcewtWOnTVXAAAAgHsiXHkI10bCVK4AAAAA90S48hAOM1xRuQIAAADcEeHKQ7j2ucpiWiAAAADglghXHsLc54ppgQAAAIBbIlx5CAeVKwAAAMCtWR6upk+frrp168rhcCgqKkpr164977m5ubmaNGmSGjRoIIfDodatW2vFihXnnJeUlKQ777xTVapUUWBgoFq2bKn169eX5ssodQ4aWgAAAABuzdJwtWjRIo0aNUoTJkxQYmKiWrdurW7duik1NbXI88eOHatZs2Zp6tSp2rp1q+6//3716dNHGzZsMM85fvy4unTpIn9/fy1fvlxbt27Vyy+/rEqVKpXVyyoVrjVX2TS0AAAAANySzTAMw6onj4qKUseOHTVt2jRJktPpVEREhB566CGNHj36nPNr1qypp556SsOHDzeP9e3bV4GBgXrnnXckSaNHj9YPP/yg//3vf8UeV3p6ukJCQpSWlqbg4OBiX6ckvfTFDk37ZreGdK6riX9vbvVwAAAAgL+Ey8kGllWucnJylJCQoOjo6LOD8fFRdHS0Vq9eXeRjsrOz5XA4Ch0LDAzUqlWrzK8/+eQTdejQQbfddpuqVaumtm3bavbs2RccS3Z2ttLT0wvd3I1rE2FasQMAAADuybJwdfToUeXn5yssLKzQ8bCwMCUnJxf5mG7dumnKlCnatWuXnE6nVq5cqcWLF+vw4cPmOXv27NGMGTPUqFEjffHFF3rggQf08MMPa968eecdS2xsrEJCQsxbREREybzIEsQ+VwAAAIB7s7yhxeV45ZVX1KhRIzVp0kQBAQEaMWKEYmJi5ONz9mU4nU61a9dOkydPVtu2bTVs2DANHTpUM2fOPO91x4wZo7S0NPN28ODBsng5l8Xc54qGFgAAAIBbsixcVa1aVb6+vkpJSSl0PCUlRdWrVy/yMaGhoVq6dKkyMjK0f/9+bd++XRUqVFD9+vXNc2rUqKFmzZoVelzTpk114MCB847FbrcrODi40M3d2F2VK1qxAwAAAG7JsnAVEBCg9u3bKz4+3jzmdDoVHx+vTp06XfCxDodD4eHhysvL00cffaRevXqZ93Xp0kU7duwodP7OnTtVp06dkn0BZYxNhAEAAAD35mflk48aNUqDBw9Whw4dFBkZqbi4OGVkZCgmJkaSNGjQIIWHhys2NlaStGbNGiUlJalNmzZKSkrSxIkT5XQ69fjjj5vXfPTRR9W5c2dNnjxZt99+u9auXavXX39dr7/+uiWvsaSwiTAAAADg3iwNV/369dORI0c0fvx4JScnq02bNlqxYoXZ5OLAgQOF1lNlZWVp7Nix2rNnjypUqKAePXpo/vz5qlixonlOx44dtWTJEo0ZM0aTJk1SvXr1FBcXp4EDB5b1yytRdjYRBgAAANyapftcuSt33OdqzZ7f1e/1n1S/anl9/dgNVg8HAAAA+EvwiH2ucHnMNVd5VK4AAAAAd0S48hDscwUAAAC4N8KVh3D4u/a5IlwBAAAA7ohw5SHsfq59rpgWCAAAALgjwpWHcFWu8p2G8vIJWAAAAIC7IVx5CNeaK4nqFQAAAOCOCFcewu73h/2+WHcFAAAAuB3ClYew2WwK8KOpBQAAAOCuCFcexHEmXLHXFQAAAOB+CFcehL2uAAAAAPdFuPIgZ8MVlSsAAADA3RCuPIirqUU2lSsAAADA7RCuPIhZucojXAEAAADuhnDlQVwbCWczLRAAAABwO4QrD0LlCgAAAHBfhCsPYjf3uaJyBQAAALgbwpUHsdOKHQAAAHBbhCsP4vArCFdsIgwAAAC4H8KVB3E1tKByBQAAALgfwpUHYRNhAAAAwH0RrjzI2YYWVK4AAAAAd0O48iCuyhVrrgAAAAD3Q7jyIGc3EaZyBQAAALgbwpUHYRNhAAAAwH0RrjwImwgDAAAA7otw5UHOrrmicgUAAAC4G8KVB7H70YodAAAAcFeEKw/CJsIAAACA+yJceZCzlSvCFQAAAOBuCFce5GzlimmBAAAAgLshXHkQNhEGAAAA3BfhyoOY4YppgQAAAIDbIVx5EHOfK1qxAwAAAG6HcOVBXJWr3HxD+U7D4tEAAAAA+CPClQdxNbSQ2EgYAAAAcDeEKw/iasUu0TEQAAAAcDeEKw/i62OTv69NEntdAQAAAO6GcOVhHGwkDAAAALglwpWHsbPXFQAAAOCWCFcextXUgsoVAAAA4F4IVx7G1Y6dhhYAAACAeyFceRg2EgYAAADck1uEq+nTp6tu3bpyOByKiorS2rVrz3tubm6uJk2apAYNGsjhcKh169ZasWLFec9/7rnnZLPZNHLkyFIYedlzVa6yqVwBAAAAbsXycLVo0SKNGjVKEyZMUGJiolq3bq1u3bopNTW1yPPHjh2rWbNmaerUqdq6davuv/9+9enTRxs2bDjn3HXr1mnWrFlq1apVab+MMuNac8UmwgAAAIB7sTxcTZkyRUOHDlVMTIyaNWummTNnqly5cpozZ06R58+fP19PPvmkevToofr16+uBBx5Qjx499PLLLxc679SpUxo4cKBmz56tSpUqXXAM2dnZSk9PL3RzV7RiBwAAANyTpeEqJydHCQkJio6ONo/5+PgoOjpaq1evLvIx2dnZcjgchY4FBgZq1apVhY4NHz5ct9xyS6Frn09sbKxCQkLMW0RERDFeTdmwm90CmRYIAAAAuBNLw9XRo0eVn5+vsLCwQsfDwsKUnJxc5GO6deumKVOmaNeuXXI6nVq5cqUWL16sw4cPm+e89957SkxMVGxs7CWNY8yYMUpLSzNvBw8eLP6LKmWuyhXTAgEAAAD3Yvm0wMv1yiuvqFGjRmrSpIkCAgI0YsQIxcTEyMen4KUcPHhQjzzyiBYsWHBOhet87Ha7goODC93clZ1W7AAAAIBbsjRcVa1aVb6+vkpJSSl0PCUlRdWrVy/yMaGhoVq6dKkyMjK0f/9+bd++XRUqVFD9+vUlSQkJCUpNTVW7du3k5+cnPz8/fffdd3r11Vfl5+en/HzPrviwiTAAAADgniwNVwEBAWrfvr3i4+PNY06nU/Hx8erUqdMFH+twOBQeHq68vDx99NFH6tWrlySpa9eu2rx5szZu3GjeOnTooIEDB2rjxo3y9fUt1ddU2ux+VK4AAAAAd+Rn9QBGjRqlwYMHq0OHDoqMjFRcXJwyMjIUExMjSRo0aJDCw8PN9VNr1qxRUlKS2rRpo6SkJE2cOFFOp1OPP/64JCkoKEgtWrQo9Bzly5dXlSpVzjnuiczKFWuuAAAAALdiebjq16+fjhw5ovHjxys5OVlt2rTRihUrzCYXBw4cMNdTSVJWVpbGjh2rPXv2qEKFCurRo4fmz5+vihUrWvQKyhabCAMAAADuyWYYhmH1INxNenq6QkJClJaW5nbNLd76Ya8mfrpVt7SqoekD2lk9HAAAAMCrXU428LhugX91drNyxbRAAAAAwJ0QrjyMg02EAQAAALdEuPIwbCIMAAAAuCfClYdxsIkwAAAA4JYIVx7GzibCAAAAgFsiXHkYcxNhpgUCAAAAboVw5WFcDS3Y5woAAABwL4QrD3N2zRWVKwAAAMCdEK48jBmu8qhcAQAAAO6EcOVh7H4FP7KcPKecTsPi0QAAAABwIVx5GFflSpJy8qleAQAAAO6CcOVhHH5nf2SsuwIAAADcB+HKw/j5+sjPxyaJjYQBAAAAd0K48kCudVdUrgAAAAD3QbjyQGc7BhKuAAAAAHdBuPJArnDFRsIAAACA+yBceSC7P9MCAQAAAHdDuPJAdj82EgYAAADcDeHKAzmoXAEAAABuh3DlgRxnKlfZVK4AAAAAt0G48kBUrgAAAAD3Q7jyQGe7BRKuAAAAAHdBuPJAZzcRZlogAAAA4C4IVx7IrFyxiTAAAADgNghXHsgVrqhcAQAAAO6DcOWB2EQYAAAAcD+EKw90dhNhwhUAAADgLghXHsjVij2baYEAAACA2yBceSCHWbkiXAEAAADugnDlgc42tGBaIAAAAOAuCFce6Ow+V4QrAAAAwF0QrjyQuc8Va64AAAAAt0G48kBmQwu6BQIAAABug3DlgdhEGAAAAHA/hCsPZK65onIFAAAAuA3ClQeiWyAAAADgfghXHujsmiumBQIAAADugnDlgex+VK4AAAAAd0O48kB/bGhhGIbFowEAAAAgEa48kt3/7I+NqYEAAACAeyBceSDHmWmBEuEKAAAAcBduEa6mT5+uunXryuFwKCoqSmvXrj3vubm5uZo0aZIaNGggh8Oh1q1ba8WKFYXOiY2NVceOHRUUFKRq1aqpd+/e2rFjR2m/jDLj72uTj63g39msuwIAAADcguXhatGiRRo1apQmTJigxMREtW7dWt26dVNqamqR548dO1azZs3S1KlTtXXrVt1///3q06ePNmzYYJ7z3Xffafjw4frpp5+0cuVK5ebm6qabblJGRkZZvaxSZbPZ2EgYAAAAcDM2w+KOCFFRUerYsaOmTZsmSXI6nYqIiNBDDz2k0aNHn3N+zZo19dRTT2n48OHmsb59+yowMFDvvPNOkc9x5MgRVatWTd99952uu+66i44pPT1dISEhSktLU3BwcDFfWelqO+lLHc/M1ZePXqerwoKsHg4AAADglS4nG1haucrJyVFCQoKio6PNYz4+PoqOjtbq1auLfEx2drYcDkehY4GBgVq1atV5nyctLU2SVLly5fNeMz09vdDN3bkqV9lUrgAAAAC3YGm4Onr0qPLz8xUWFlboeFhYmJKTk4t8TLdu3TRlyhTt2rVLTqdTK1eu1OLFi3X48OEiz3c6nRo5cqS6dOmiFi1aFHlObGysQkJCzFtERMSVvbAyYE4LzGPNFQAAAOAOLF9zdbleeeUVNWrUSE2aNFFAQIBGjBihmJgY+fgU/VKGDx+uLVu26L333jvvNceMGaO0tDTzdvDgwdIafomx+xW8XjYSBgAAANyDpeGqatWq8vX1VUpKSqHjKSkpql69epGPCQ0N1dKlS5WRkaH9+/dr+/btqlChgurXr3/OuSNGjNBnn32mb775RrVq1TrvOOx2u4KDgwvd3J2dhhYAAACAW7E0XAUEBKh9+/aKj483jzmdTsXHx6tTp04XfKzD4VB4eLjy8vL00UcfqVevXuZ9hmFoxIgRWrJkib7++mvVq1ev1F6DVRxnKlfZTAsEAAAA3IKf1QMYNWqUBg8erA4dOigyMlJxcXHKyMhQTEyMJGnQoEEKDw9XbGysJGnNmjVKSkpSmzZtlJSUpIkTJ8rpdOrxxx83rzl8+HAtXLhQH3/8sYKCgsz1WyEhIQoMDCz7F1kKaMUOAAAAuBfLw1W/fv105MgRjR8/XsnJyWrTpo1WrFhhNrk4cOBAofVUWVlZGjt2rPbs2aMKFSqoR48emj9/vipWrGieM2PGDEnSDTfcUOi55s6dqyFDhpT2SyoTDn/WXAEAAADuxPJ9rtyRJ+xz9fC7G/TJpkMae0tT3XvtuevNAAAAAFw5j9nnCsXnqlxl5zEtEAAAAHAHhCsPdXYTYaYFAgAAAO6AcOWhzm4iTOUKAAAAcAeEKw/lYBNhAAAAwK0QrjzU2U2ECVcAAACAOyBceSi7Hw0tAAAAAHdCuPJQDipXAAAAgFshXHmos+GKyhUAAADgDghXHspOQwsAAADArRCuPJS5zxVrrgAAAAC3QLjyUA5/KlcAAACAOyFceSgqVwAAAIB7IVx5KNZcAQAAAO6FcOWhqFwBAAAA7oVw5aEcfuxzBQAAALgTwpWH+mNDC8MwLB4NAAAAAMKVh7KfqVw5DSk3n3AFAAAAWI1w5aHs/md/dFl5TA0EAAAArEa48lB2Px/ZbAX/zs6lqQUAAABgNcKVh7LZbLRjBwAAANwI4cqDnW3HTrgCAAAArEa48mBnK1dMCwQAAACsVqxwdfDgQf3222/m12vXrtXIkSP1+uuvl9jAcHFUrgAAAAD3UaxwNWDAAH3zzTeSpOTkZP3f//2f1q5dq6eeekqTJk0q0QHi/M5uJEzlCgAAALBascLVli1bFBkZKUl6//331aJFC/34449asGCB3nrrrZIcHy7gjxsJAwAAALBWscJVbm6u7Ha7JOmrr77S3//+d0lSkyZNdPjw4ZIbHS7ITuUKAAAAcBvFClfNmzfXzJkz9b///U8rV67UzTffLEk6dOiQqlSpUqIDxPm5NhJmzRUAAABgvWKFq+eff16zZs3SDTfcoP79+6t169aSpE8++cScLojS52poQeUKAAAAsJ5fcR50ww036OjRo0pPT1elSpXM48OGDVO5cuVKbHC4sLPhisoVAAAAYLViVa5Onz6t7OxsM1jt379fcXFx2rFjh6pVq1aiA8T5mftcMS0QAAAAsFyxwlWvXr309ttvS5JOnDihqKgovfzyy+rdu7dmzJhRogPE+bm6BWYzLRAAAACwXLHCVWJioq699lpJ0ocffqiwsDDt379fb7/9tl599dUSHSDOz9znisoVAAAAYLlihavMzEwFBQVJkr788kv94x//kI+Pj66++mrt37+/RAeI83OtuaJyBQAAAFivWOGqYcOGWrp0qQ4ePKgvvvhCN910kyQpNTVVwcHBJTpAnJ+55oqGFgAAAIDlihWuxo8fr8cee0x169ZVZGSkOnXqJKmgitW2bdsSHSDOj26BAAAAgPsoViv2f/7zn7rmmmt0+PBhc48rSeratav69OlTYoPDhZkNLfKYFggAAABYrVjhSpKqV6+u6tWr67fffpMk1apViw2Ey5idyhUAAADgNoo1LdDpdGrSpEkKCQlRnTp1VKdOHVWsWFHPPPOMnE6qKGXl7LRAvucAAACA1YpVuXrqqaf05ptv6rnnnlOXLl0kSatWrdLEiROVlZWlZ599tkQHiaKxiTAAAADgPooVrubNm6c33nhDf//7381jrVq1Unh4uB588EHCVRmhFTsAAADgPoo1LfDYsWNq0qTJOcebNGmiY8eOXfGgcGkcVK4AAAAAt1GscNW6dWtNmzbtnOPTpk1Tq1atLvt606dPV926deVwOBQVFaW1a9ee99zc3FxNmjRJDRo0kMPhUOvWrbVixYoruqanonIFAAAAuI9iTQt84YUXdMstt+irr74y97havXq1Dh48qGXLll3WtRYtWqRRo0Zp5syZioqKUlxcnLp166YdO3aoWrVq55w/duxYvfPOO5o9e7aaNGmiL774Qn369NGPP/5o7rF1udf0VHZ/NhEGAAAA3EWxKlfXX3+9du7cqT59+ujEiRM6ceKE/vGPf+iXX37R/PnzL+taU6ZM0dChQxUTE6NmzZpp5syZKleunObMmVPk+fPnz9eTTz6pHj16qH79+nrggQfUo0cPvfzyy8W+pqdy+J2pXLHPFQAAAGC5Yu9zVbNmzXMaV2zatElvvvmmXn/99Uu6Rk5OjhISEjRmzBjzmI+Pj6Kjo7V69eoiH5OdnS2Hw1HoWGBgoFatWnVF18zOzja/Tk9Pv6TxW83BPlcAAACA2yhW5aqkHD16VPn5+QoLCyt0PCwsTMnJyUU+plu3bpoyZYp27dolp9OplStXavHixTp8+HCxrxkbG6uQkBDzFhERUQKvrvQ5zkwLzHMaysunegUAAABYydJwVRyvvPKKGjVqpCZNmiggIEAjRoxQTEyMfHyK/1LGjBmjtLQ083bw4MESHHHpsZ+ZFihJWUwNBAAAACxlabiqWrWqfH19lZKSUuh4SkqKqlevXuRjQkNDtXTpUmVkZGj//v3avn27KlSooPr16xf7mna7XcHBwYVunsC1ibAkZTM1EAAAALDUZa25+sc//nHB+0+cOHFZTx4QEKD27dsrPj5evXv3liQ5nU7Fx8drxIgRF3ysw+FQeHi4cnNz9dFHH+n222+/4mt6Gh8fmwL8fJST56RyBQAAAFjsssJVSEjIRe8fNGjQZQ1g1KhRGjx4sDp06KDIyEjFxcUpIyNDMTExkqRBgwYpPDxcsbGxkqQ1a9YoKSlJbdq0UVJSkiZOnCin06nHH3/8kq/pTRyucEXlCgAAALDUZYWruXPnlvgA+vXrpyNHjmj8+PFKTk5WmzZttGLFCrMhxYEDBwqtp8rKytLYsWO1Z88eVahQQT169ND8+fNVsWLFS76mN7H7+0pZeYQrAAAAwGI2wzAMqwfhbtLT0xUSEqK0tDS3X3917Qtf6+Cx01r8YGe1q13J6uEAAAAAXuVysoHHdQtEYa6NhKlcAQAAANYiXHk410bC2bk0tAAAAACsRLjycK527FSuAAAAAGsRrjycq3KVlUe4AgAAAKxEuPJwDv+CHyHTAgEAAABrEa48nN2fhhYAAACAOyBceTizW2AelSsAAADASoQrD2f3p6EFAAAA4A4IVx7OVbnKpnIFAAAAWIpw5eEcVK4AAAAAt0C48nBmK3a6BQIAAACWIlx5ONcmwtlUrgAAAABLEa48nKtyxZorAAAAwFqEKw/HmisAAADAPRCuPJy55iqPcAUAAABYiXDl4VxrrmhoAQAAAFiLcOXh7OaaKypXAAAAgJUIVx7OtYkwlSsAAADAWoQrD0dDCwAAAMA9EK48nJ3KFQAAAOAWCFcezlW5YhNhAAAAwFqEKw/HJsIAAACAeyBceThXuMrJdyrfaVg8GgAAAOCvi3Dl4VzTAiXasQMAAABWIlx5OFdDC4mmFgAAAICVCFceztfHJn9fmyQqVwAAAICVCFdegI2EAQAAAOsRrryA3d8VrqhcAQAAAFYhXHkBu1/Bj5FwBQAAAFiHcOUFzI2E2esKAAAAsAzhygs4mBYIAAAAWI5w5QXOhisqVwAAAIBVCFdewLXmilbsAAAAgHUIV17AVbnKpnIFAAAAWIZw5QVcDS2yqFwBAAAAliFceYGzmwgTrgAAAACrEK68gN1VuWJaIAAAAGAZwpUXsFO5AgAAACxHuPICZkMLNhEGAAAALEO48gJmQwsqVwAAAIBlCFdegE2EAQAAAOtZHq6mT5+uunXryuFwKCoqSmvXrr3g+XFxcWrcuLECAwMVERGhRx99VFlZWeb9+fn5GjdunOrVq6fAwEA1aNBAzzzzjAzDKO2XYhnXJsK0YgcAAACs42flky9atEijRo3SzJkzFRUVpbi4OHXr1k07duxQtWrVzjl/4cKFGj16tObMmaPOnTtr586dGjJkiGw2m6ZMmSJJev755zVjxgzNmzdPzZs31/r16xUTE6OQkBA9/PDDZf0SywSbCAMAAADWs7RyNWXKFA0dOlQxMTFq1qyZZs6cqXLlymnOnDlFnv/jjz+qS5cuGjBggOrWraubbrpJ/fv3L1Tt+vHHH9WrVy/dcsstqlu3rv75z3/qpptuumhFzJO51lxlU7kCAAAALGNZuMrJyVFCQoKio6PPDsbHR9HR0Vq9enWRj+ncubMSEhLMoLRnzx4tW7ZMPXr0KHROfHy8du7cKUnatGmTVq1ape7du593LNnZ2UpPTy908yRsIgwAAABYz7JpgUePHlV+fr7CwsIKHQ8LC9P27duLfMyAAQN09OhRXXPNNTIMQ3l5ebr//vv15JNPmueMHj1a6enpatKkiXx9fZWfn69nn31WAwcOPO9YYmNj9fTTT5fMC7MAmwgDAAAA1rO8ocXl+PbbbzV58mS99tprSkxM1OLFi/X555/rmWeeMc95//33tWDBAi1cuFCJiYmaN2+eXnrpJc2bN++81x0zZozS0tLM28GDB8vi5ZQYV+WKaYEAAACAdSyrXFWtWlW+vr5KSUkpdDwlJUXVq1cv8jHjxo3TXXfdpXvvvVeS1LJlS2VkZGjYsGF66qmn5OPjo3//+98aPXq07rjjDvOc/fv3KzY2VoMHDy7yuna7XXa7vQRfXdmy04odAAAAsJxllauAgAC1b99e8fHx5jGn06n4+Hh16tSpyMdkZmbKx6fwkH19C4KFq9X6+c5xOr03eLCJMAAAAGA9S1uxjxo1SoMHD1aHDh0UGRmpuLg4ZWRkKCYmRpI0aNAghYeHKzY2VpLUs2dPTZkyRW3btlVUVJR2796tcePGqWfPnmbI6tmzp5599lnVrl1bzZs314YNGzRlyhTdfffdlr3O0manoQUAAABgOUvDVb9+/XTkyBGNHz9eycnJatOmjVasWGE2uThw4EChKtTYsWNls9k0duxYJSUlKTQ01AxTLlOnTtW4ceP04IMPKjU1VTVr1tR9992n8ePHl/nrKytnW7F7b3UOAAAAcHc2wzWfDqb09HSFhIQoLS1NwcHBVg/noo6eylaH/3wlSdob20M2m83iEQEAAADe4XKygUd1C0TRHGcaWkhUrwAAAACrEK68gN3v7I+RdVcAAACANQhXXsDf10e+PgVTAalcAQAAANYgXHkJhx/t2AEAAAArEa68hIONhAEAAABLEa68xNlwReUKAAAAsALhykvYmRYIAAAAWIpw5SXsZypXNLQAAAAArEG48hIOfypXAAAAgJUIV17C4XdmzRWVKwAAAMAShCsvYadyBQAAAFiKcOUlXJUr1lwBAAAA1iBceQnXmqtsKlcAAACAJQhXXoJ9rgAAAABrEa68xNl9rpgWCAAAAFiBcOUlHOY+V1SuAAAAACsQrryE3ZwWSOUKAAAAsALhykuwiTAAAABgLcKVl7CziTAAAABgKcKVl6AVOwAAAGAtwpWXcFC5AgAAACxFuPIS7HMFAAAAWItw5SWYFggAAABYi3DlJcyGFrRiBwAAACxBuPISZuWKTYQBAAAASxCuvISDTYQBAAAASxGuvIS5iTCVKwAAAMAShCsvcXbNFeEKAAAAsALhykvYzTVXThmGYfFoAAAAgL8ewpWXcK25MgwpJ591VwAAAEBZI1x5CceZaYESTS0AAAAAKxCuvIS/r002W8G/2UgYAAAAKHuEKy9hs9nM6lV2HpUrAAAAoKwRrryI2Y6dyhUAAABQ5ghXXoSNhAEAAADrEK68iN2PjYQBAAAAqxCuvIircpVN5QoAAAAoc4QrL2I3pwVSuQIAAADKGuHKiziYFggAAABYhnDlRWhoAQAAAFiHcOVFzIYWTAsEAAAAyhzhyouYDS3YRBgAAAAoc5aHq+nTp6tu3bpyOByKiorS2rVrL3h+XFycGjdurMDAQEVEROjRRx9VVlZWoXOSkpJ05513qkqVKgoMDFTLli21fv360nwZboFNhAEAAADr+Fn55IsWLdKoUaM0c+ZMRUVFKS4uTt26ddOOHTtUrVq1c85fuHChRo8erTlz5qhz587auXOnhgwZIpvNpilTpkiSjh8/ri5duujGG2/U8uXLFRoaql27dqlSpUpl/fLK3NlW7IQrAAAAoKxZGq6mTJmioUOHKiYmRpI0c+ZMff7555ozZ45Gjx59zvk//vijunTpogEDBkiS6tatq/79+2vNmjXmOc8//7wiIiI0d+5c81i9evUuOI7s7GxlZ2ebX6enp1/R67LK2U2EmRYIAAAAlDXLpgXm5OQoISFB0dHRZwfj46Po6GitXr26yMd07txZCQkJ5tTBPXv2aNmyZerRo4d5zieffKIOHTrotttuU7Vq1dS2bVvNnj37gmOJjY1VSEiIeYuIiCiBV1j2qFwBAAAA1rEsXB09elT5+fkKCwsrdDwsLEzJyclFPmbAgAGaNGmSrrnmGvn7+6tBgwa64YYb9OSTT5rn7NmzRzNmzFCjRo30xRdf6IEHHtDDDz+sefPmnXcsY8aMUVpamnk7ePBgybzIMkYrdgAAAMA6lje0uBzffvutJk+erNdee02JiYlavHixPv/8cz3zzDPmOU6nU+3atdPkyZPVtm1bDRs2TEOHDtXMmTPPe1273a7g4OBCN09kZxNhAAAAwDKWrbmqWrWqfH19lZKSUuh4SkqKqlevXuRjxo0bp7vuukv33nuvJKlly5bKyMjQsGHD9NRTT8nHx0c1atRQs2bNCj2uadOm+uijj0rnhbgRu1m5IlwBAAAAZc2yylVAQIDat2+v+Ph485jT6VR8fLw6depU5GMyMzPl41N4yL6+BYHCMAxJUpcuXbRjx45C5+zcuVN16tQpyeG7JceZyhX7XAEAAABlz9JugaNGjdLgwYPVoUMHRUZGKi4uThkZGWb3wEGDBik8PFyxsbGSpJ49e2rKlClq27atoqKitHv3bo0bN049e/Y0Q9ajjz6qzp07a/Lkybr99tu1du1avf7663r99dcte51lxUHlCgAAALCMpeGqX79+OnLkiMaPH6/k5GS1adNGK1asMJtcHDhwoFClauzYsbLZbBo7dqySkpIUGhqqnj176tlnnzXP6dixo5YsWaIxY8Zo0qRJqlevnuLi4jRw4MAyf31ljYYWAAAAgHVshms+HUzp6ekKCQlRWlqaRzW3+H7nEQ2as1ZNqgdpxcjrrB4OAAAA4PEuJxt4VLdAXJircpXDmisAAACgzBGuvIjD/0wrdtZcAQAAAGWOcOVFzDVXVK4AAACAMke48iLmJsJUrgAAAIAyR7jyIq7KFftcAQAAAGWPcOVFHH4F4SrfaSg3n4AFAAAAlCXClRex+5/9cTI1EAAAAChbhCsv4lpzJbGRMAAAAFDWCFdexGaz0dQCAAAAsAjhysvQ1AIAAACwBuHKy7CRMAAAAGANwpWXOVu5IlwBAAAAZYlw5WXOrrliWiAAAABQlghXXobKFQAAAGANwpWXcW0kTOUKAAAAKFuEKy9jp6EFAAAAYAnClZexU7kCAAAALEG48jKuVuysuQIAAADKFuHKy7gaWlC5AgAAAMoW4crLsIkwAAAAYA3ClZcx11wxLRAALirtdK4WrTvAB1IAgBJBuPIy5porpgUCwEX9d+VOPfHRZk37erfVQwEAeAHClZdx7XNFQwsAuLif9vwuSVrxS7LFIwEAeAPClZehoQUAXJpT2XnamXJSkrQ79ZT2/55h8YgAAJ6OcOVlaGgBAJfm54Mn5DTOfh2/LdW6wQAAvALhysuc3USYcAUAF7Lh4AlJkr+vTZIUvz3FwtEAALwB4crL2M1NhJkWCAAXkrj/uCSpf2RtSdKaPceUnpVr5ZAAAB6OcOVlzq65onIFAOdjGIZZuerTNlz1Q8srz2no+51HrB0YAMCjEa68DA0tAODi9v+eqWMZOQrw9VGzmsGKbhomiXVXAIArQ7jyMna/Mw0taMUOAOe14WDBlMAW4cGy+/mqa5NqkqRvdqQqL58PpwAAxUO48jKuyhWbCAPA+SXuPyFJalu7kiSpfZ1KCgn014nMXCUeOGHdwAAAHo1w5WUcZkMLKlcAcD6uylW7M+HKz9dHNzYOlUTXQABA8RGuvIzDjzVXAHAhmTl52na4YPPgtrUrmse7su4KAHCFCFdexs4mwgBwQZt/S1O+01D1YIdqVgw0j193Vaj8fGzanXpK+3/PsHCEAABPRbjyMq7KVZ7TYFE2ABTBtabqj1UrSQoJ9FfHupUlSV9RvQIAFAPhysu4GlpIbCQMAEXZcKDweqs/6tq0oGtg/DbWXQEALh/hysu4WrFLTA0EgD8zDOO8lStJ5n5Xa/ceU3pWbhmODADgDQhXXsbHx6YAX9deV1SuAOCPfjt+WkdPZcvf16YW4SHn3F+3ank1CC2vPKeh73YcsWCEAABPRrjyQq6mFtlUrgCgkMQzUwKb1QguNI36j1zVq6+3s+4KAHB5CFdeyPUHA+3YAaCwDeaUwHPXW7m4WrJ/syOVxkAAgMtCuPJCro2Es9hIGAAKcTWzKGq9lUu72hUVEuivE5m55vosAAAuhVuEq+nTp6tu3bpyOByKiorS2rVrL3h+XFycGjdurMDAQEVEROjRRx9VVlZWkec+99xzstlsGjlyZCmM3D2d3UiYcAUALlm5+frlULqkojsFuvj5+ujGxqGS6BoIALg8loerRYsWadSoUZowYYISExPVunVrdevWTampRc91X7hwoUaPHq0JEyZo27ZtevPNN7Vo0SI9+eST55y7bt06zZo1S61atSrtl+FWzq65YjoLALj8cihNeU5DVSvYVatS4AXPdU0N/IpwBQC4DJaHqylTpmjo0KGKiYlRs2bNNHPmTJUrV05z5swp8vwff/xRXbp00YABA1S3bl3ddNNN6t+//znVrlOnTmngwIGaPXu2KlU6/yeU3shVucpmWiAAmBL3n5BUMCXQZrNd8NzrG4fKz8emX49kaN/RjDIYHQDAG1garnJycpSQkKDo6GjzmI+Pj6Kjo7V69eoiH9O5c2clJCSYYWrPnj1atmyZevToUei84cOH65Zbbil07fPJzs5Wenp6oZsno6EFAJxrw8Hzbx78Z8EOf0XWqyxJiqdrIADgElkaro4ePar8/HyFhYUVOh4WFqbk5OQiHzNgwABNmjRJ11xzjfz9/dWgQQPdcMMNhaYFvvfee0pMTFRsbOwljSM2NlYhISHmLSIiovgvyg2YDS1YcwUApj9Wri6Fa2og664AAJfK8mmBl+vbb7/V5MmT9dprrykxMVGLFy/W559/rmeeeUaSdPDgQT3yyCNasGCBHA7HJV1zzJgxSktLM28HDx4szZdQ6uw0tACAQg6nnVZyepZ8fWxqVevczYOLEt20miRp7d5jSs/KLc3hAQC8hJ+VT161alX5+voqJaXwp4IpKSmqXr16kY8ZN26c7rrrLt17772SpJYtWyojI0PDhg3TU089pYSEBKWmpqpdu3bmY/Lz8/X9999r2rRpys7Olq9v4Y0j7Xa77HZ7Cb8665gNLfKYFggA0tmqVZPqQSoXcGn/6atTpbwahJbXr0cy9N2OI+rZumYpjhAA4A0srVwFBASoffv2io+PN485nU7Fx8erU6dORT4mMzNTPj6Fh+0KS4ZhqGvXrtq8ebM2btxo3jp06KCBAwdq48aN5wQrb8SaKwAozLW/1aWst/qjaKYGAgAug6WVK0kaNWqUBg8erA4dOigyMlJxcXHKyMhQTEyMJGnQoEEKDw8310/17NlTU6ZMUdu2bRUVFaXdu3dr3Lhx6tmzp3x9fRUUFKQWLVoUeo7y5curSpUq5xz3VuY+V3QLBABJUuIlbB5clK5NwzTr+z36ZscR5eU75efrcbPpAQBlyPJw1a9fPx05ckTjx49XcnKy2rRpoxUrVphNLg4cOFCoUjV27FjZbDaNHTtWSUlJCg0NVc+ePfXss89a9RLcjp2GFgBgys7L15ZL2Dy4KO1qV1TFcv46kZmrhP3HFVW/SmkMEQDgJSwPV5I0YsQIjRgxosj7vv3220Jf+/n5acKECZowYcIlX//P1/B2Z/e5YlogAGw9lK6cPKcqlw9QnSrlLuuxfr4+urFxNS3ZkKSvt6cSrgAAF8T8Bi9EK3YAOGvDgROSpLYRF988uChdz3QN/Ip1VwCAiyBceSFXQ4tsGloAQLHXW7lcd1Wo/Hxs+vVIhvYdzSjBkQEAvA3hygvZ/ahcAYCLq3J1ueutXIId/oqsV1kS1SsAwIURrryQWblizRWAv7jU9CwlnTgtH5vUKqJisa/T1WzJnlpCIwMAeCPClRdizRUAFEg8U7W6KixIFezF7+EUfWbd1bp9x5R2OrckhgYA8EKEKy9k92efKwCQzm4e3LaYUwJd6lQpr4bVKijPaei7nUdKYmgAAC9EuPJC5ibCNLQA8Bd3dr1VxSu+lqtr4NesuwIAnAfhygu5NhHOpnIF4C8sN9+pn5NOSLryypUkRZ9Zd/XNjiPKy+fDKwDAuQhXXojKFQBI2w+fVFauUyGB/qpftfwVX69tREVVLOevtNO5Sth/vARGCADwNoQrL0RDCwA4u79Vm4iK8vG5/M2D/8zP10c3Ni6YGhi/na6BAIBzEa68EJsIA8DZZhbF3d+qKK51V+x3BQAoCuHKC7k2Ec7JdyrfaVg8GgCwhqsNe9sSaGbhct1VofLzsWnPkQztPZpRYtcFAHgHwpUXclWuJCmHjYQB/AUdPZWtA8cyZbNJbUowXAU7/BVVv7IkKZ7qFQDgTwhXXuiP4Yp1VwD+ijaeqVo1DK2gYId/iV67a5OCroHx21h3BQAojHDlhXx9bPL3LVi8zUbCAP6KEkthvZWLa93Vun3HlHY6t8SvDwDwXIQrL2WnHTuAv7ANpbDeyqVOlfJqWK2C8pyGvtt5pMSv7w2cTkMHj2XqyMlsGYb3rv39/OfDGrP4Zx3PyLF6KADchJ/VA0DpcPj76FQ2GwkDF7JiS7LST+fqtg61ZLNdeatuuIe8fKc2/XZCktSuTslXrqSC6tXu1FOK35aiv7euWSrP4SmcTkP7j2Vqc1KaNv92QpuT0vRLUrpOZudJKmiyFF4xUOGVAlUzpOB/XV+HVwxU9RCH/H0v/bPe0zn5+j0jW8czcvV7RraOZeSYt6xcp+65tp7CKwaW1ss1/bj7qB56N1FOQ/rt+Gm9FRMp3xJo+Q/AsxGuvBSVK+9hGIa+2ZGqRtWCFFG5nNXD8Rofb0zSI+9tlCQdz8zRfdc3sHZAKDE7U04pMydfQXY/NQytUCrPcVOzMM36bo8++/mwbu8QoS4Nq5bK87gbwzB04Fimfv4tTVuS0gr+91CaTmblnXNugK+Pcp1OZec5tedohvacp7uij02qHuwoFLrKBfgVCk1/vJ2+yFri73cd0eIHO5f4Wrs/Opx2Wg+9u0Guhrz/23VUcV/t1L9ualxqzwnAMxCuvBQbCXuPl77coenf/KqQQH99eH8nNQoLsnpIHm/DgeP694c/m18/t2K7GoRWUHSzMAtHhZJibh5cu2Q2Dy5Ku9qV1KdtuJZsSNKDCxK1dHgX1atavlSeyyVh/3G9+MV29Y+srV5twkv1uVyycvP19fZU/fxbmjYnndDm39KUXlSQ8vNRsxrBalUrRC3CQ9SqVogahlaQISk5LUu/HT+tpBOnlXT8tJJOZJr/PnQiSzn5Th1Ky9KhtCyt0/FLGpe/r02Vyweocnm7qpQPUKXyAapSPkDLtxzW7tRTevjdDXpzcMdSqSRl5+XrgXcS9XtGjprVCNaQznX1+Ec/a+rXu9W6VkV+j6DU5eU7tfvIKTUOC2LWhRsiXHmps5UrwpUnm796n6Z/86skKe10rgbPWavFD3ZR9RCHxSPzXIdOnNaw+QnKyXMqumk1VQt2aOGaA3rkvQ366MHOalI92Oohuo3svHz52mzyu4wpW+7AXG8VUbHUnsNmsyn2Hy217/cMbThwQvfMW6clD3ZRSGDpVEu2HkrXkLlrdTIrT2v3HlOAr4+6t6xRKs/lcjIrV3e+sUabfksrdDzA10dNawSpZa0QtQwPUcvwimoUVuG8U/siKpc7b9Xd6TR0NCP7TOg6bf7v6Zz8M+HpbHCq/IdbBbtfkX9U9m1XS7fN+lHf7jiiycu2adytza78G/En//lsmzYePKFgh59m3tletauU09bD6Xrrx3169P2N+nTENapbykEbJc8wDGXlOhUY4Hvxky10OidfQ+au1Zq9x9Q/MkKT+7QkYLkZwpWXclWustnnymOt2JKs8Z/8Ikm677r6WrktRXuOZGjI3LV6//5OpTrlxVtl5uRp6NvrdeRktppUD1LcHW1l9/PRvqMZ+vHX33XPW+v18YguqlrBbvVQLbf5tzTFvLVO5e2+mhcT6VF/LG44U7lqW0rrrVwc/r56/a4O6jVtlfYcydCIhYmaO6RjiYfRfUczNGhOQbAKCfRX2ulcPfLeRgU5/HVNo9KZjpiZk6e731qnTb+lKSTQX7e0qqFW4QVVqavCghTgVzKv0cfHpmpBDlULcqhtCXR2bFkrRC/f1kbDFybqzVV71ahaBd0RWbsERlrgo4TfNP+n/ZKkV+5oq9pVCkLjkz2aaktSmtbvP67730nQkge7uP0f6TjrVHaeHngnQWv3HtNzfVuqT9taVg+pSNl5+Ro2f73W7D0mSXp37UHZ/Xw1oWczApYb8ayPI3HJXHtdUbnyTOv2HdPD722QYUgDomprdPcmmhcTqdAgu7Ynn9R9byfQrOQyOZ2G/vX+Jv1yKF1Vygdo9qAOqmD3k7+vj14b2E51q5RT0onTun8+39uE/cc0YPZPOnoqW/t/z9Tts1ZrV8pJq4d1SY5n5Jhre9rUqljqzxcaZNfswR0U6O+r/+06qmc+21qi109Jz9Kdb67R0VPZalojWN8+doO6t6iunHynhs1fbwbJkpSVm6+hb6/Xun3HFeTw04J7ozS5T0vdEVlbLcJDSixYlZZbWtXQo9FXSZLGLt2in/b8XiLX3XooXU8u2SxJerhrI93YpJp5X4Cfj6YPbKeqFQp+Rz+5ZLNXd0n0Jmmnc3XXm2v0v11HlZ3n1Kj3N+ndtQesHtY5cvOdGr5gg/6366gC/X113/X1JUlv/bhPz6/YwfvNjbj3b0gUmytcZdPQwuPsSjmpe+etPzNtLUyT/t5cNptNEZXLae6Qjqpg99PqPb/rX+9vktPJL9NLFffVTi3fkix/X5tm3tW+0DSliuUC9Mbgjgpy+Gn9/uN6asmWv+x/qH7cfVR3vblWJ7PzFFm3sppUD1LqyWz1e/0n/XIo7eIXsNjGgyckSfWrllel8gFl8pzNa4bov/3aSJLmrd6vd85UNq7Uicwc3fXmGv12/LTqVCmneXd3VKXyAYq7o42uaVhVmTn5inlrnXaWYPDNyXPqwQWJ+mH37yof4Kt5d0eqRXhIiV2/rDzctaFubVVDeU5DD7yToAO/Z17R9dIyc3X/OwnKznPq+qtC9UjXRuecExbs0LQBbeXrY9OSDUkl9j5A6TmWkaMBs3/ShgMnFBLor56ta8owpDGLN2vOqr1WD8+Ul+/UyEUb9dW2FAX4+ejNwR00pntTPdO7hSRp5ne/aurXuy0eJVwIV17KbGjxF/8E3tMkp2Vp8Jy1Sjudq3a1K2pq/7aFphi1CA/RzDvby8/Hps9+PqzJy7ZZOFrP8fHGJL165j88k/u0VMe6lc85p2G1Cpo+oJ18bNKHCb9p9v/2lPUwLffNjlTFvLVOmTn5urZRVc27O1LvDr1aLcNDdCwjR/1f/6lUKiUlyZwSWAqbB1/IzS2q69/dCjrFTfjkF/24++gVXS8zJ+9McDqlsGC73rknStWCCtZa2v18Neuu9modUVEnMgs+dT947MrCg1TwB9zD727Q19tTZffz0ZtDOpbKJsxlwWaz6aXbWqtVrRAdz8zVPfPW6WRW8TZ8djoNjXp/ow4cy1StSoF65Y42522UcXX9KhrTvYkkadJnW5Ww373///JXlpqepTteX23OZnhv2NV69Y42uu+6gorQpM+2avo31gcWp9PQ4x/9rM9/Pix/X5tm3dlenc90J73r6joae0tTSdKUlTv1+ve/WjlUnEG48lKuhhZUrjxHelauhsxdq0NpWaofWl5vDu5Y5Jz9axpV1Uu3tZYkvbFqr974C4aAy7Hx4AmzM+B919XXbR0iznvudVeFavyZBfCxy7crfltKmYzRHazYcljD3l6v7DONPmYP6qDAAF9VKh+gBUOj1L5OJaVn5enON9ZoTQlNsyoNiaW4efDFPHhDA/VuU1P5TkMPLEjU3vO0Hr+Y7Lx83Tc/wfw0/e27o85pCFHe7qe3hnRUo2oVlJKerbveXKMjJ7OLPfZ8p6F/fbBJK35JVoCvj2YP6qCr61cp9vXcgcPfV7MHdVBYsF27znQQzC9GtX/6N7sVvz1VAX4+mnlne1Usd+GK6D3X1FOPltWVm29o+IJEHT1V/J8LSsehE6fV7/WfzA8vFt3XSU1rBMtms2l09yYaGV1QmXzxix16+UvrptwZhqGxH2/R4sQk+frYNLV/u0LTUSXp3mvr67GbCqbBTl62XfNX77NgpPgjwpWXohW7Z8nOy9ewt9dre/JJhQbZNS8m8oJTmnq3DTc/Hf3P59v0yaZDZTVUj3I47bSGvl0wxbJrk2p6/OYmF33M4M51NSCqtgxDevjdDdqR7Blrja7ExxuTNHzhBuXmG7qlVQ3NuLO9ObVYkoId/nr77kh1blBFGTn5Gjx3rb7fecTCERct32mY0wKtqLjYbDY917eV2kRUVNrpgmpJ2unLq5bkOw2NWrRJ/9t1VOUCfDU3pqMaVy96+4VK5QM0/54o1aoUqH2/Z2rwnLVKL0Z1xuk09NSSzfp44yH5+dj02sB2uu6q0Mu+jjsKC3Zo9qAOcvj76JsdRxR7mdX+73Ye0ZSvdkqS/tO7xSVNkbTZbHrhn63VILS8ktOz9NDCDcrL54NOd7H/9wzdNnO19h7NUHjFQL1/Xyc1rHZ2PzybzaaR0VeZ/42d+vVu/efzbWUesAzD0DOfbdPCNQdks0lTbm+tm1tUL/LcEX9rpOE3FuzVOO7jX/T+uoNlOVT8CeHKS5mt2JkW6PZcjRZ+2nNMFex+eium4yVtFjzsuvoa0rmuJOlf72+84mlI3uaPnQEbhwXplf5tL2nPG5vNpqf/3lyd6hcEiXvmrdPvXvzJ86J1BzRy0UblOw31bVdLr97RtsiW2uXtfpozpKNubByqrFyn7p23Xl9tda/K3u7UUzqVnadyAb66Kqx0Ng++GIe/r14f1F41QxxmB8FL/cPaMAyNXbpFn28+M/3nrvYXDYnVQxyaf0+UqlYI0NbD6br3rfU6nXPpv/cNw9Ckz7bqvXUH5WOT4u5o43X7NLWqVbFQtX/RuktrVnDwWKYeOdNYqH9khG6/QNX7zyrY/TTrrvYqH+Cr1Xt+14tf7ijW2FGydqee0u2zVivpxGnVq1peH9zfSXWqFN0J9b7rG2hSr+aSpDdX7dXYpVvKdJ3zy1/u1JwfCtZ9Pf+PVhfd2+6xmxrr7i71JElPLP5ZH29MKvUxomiEKy91tlsgn5a5u2eXbdNnP5/9Y6p5zUtbPG6z2TT+1ma6pWUN5eYbum9+grYeSi/l0XoGp9PQYx9s0pakdFUuH6A3Bhd0BrxUrg6CdaqU02/HT59ZyO59H1S89cNePfHRZhmGNDCqtl78Z6sLBlCHv69m3dVBNzcv6FZ3/zsJ+uxn96mautZbta5V0dK9uaoFOQp1EPzP55dWLXnxix16d23Bp9Rx/drq2kaXVj2qV7W85t0dqSC7n9buO6bhCxOVewmBzjAMPbdiu976cV/B8/+ztW5tVfOSntPT3NqqpjnVa+zSLRed2pqVm68HFyTqRGauWtUK0YSezS/7ORtWC9KLZ0LdrO/2aPnmw5c/cJSYbYfT1W/WaqWkZ+uqsApadN/Vqlkx8IKPGdSprl7o20o2m7RgzQE99uGmMqlCTvt6l6adWe81qVdz3d7x4sHeZrNp3K1NzZkXo97fpBVbkkt7qCgC4cpL2f2YFugJ3vjfHr15piPRS7e1VpeGl7dnjY+PTS/f3lpR9SrrZHaehsxdq9+OX/nCdk8XF79LyzYnm4H1UiqBf1apfIDeHNxBQXY/rdt3XGO9rIPgjG9/1cRPC9qG33tNPf2ndwv5XEJlL8DPR9MGtFWvNjWV5zT08Lsb9GHCb6U93EuSaDazqGjtQOTqIFjwh/VbP+7TgjUX7hw3+/s9eu3bgsXok/u01C2tLm+D4OY1Q/TmkI6y+/no6+2p+vcHF+8m+kr8Ls36rmDN5rN9Wqhve/fc26ekPNK1kW5pVfBh1P0X6SA48ZNftDkpTZXK+eu1ge0KTZO9HD1a1tDQawuqCf/+8GftTj1VrOvgymw6eEJ3vP6Tfs/IUfOawXpvWCezQczF3N4xQnH9CpqYLE5M0iPvbVROKe4h+sb/9uilLwumoj7Zo4kGdap7yY+12Wz6T68W+ke7cOU7DT30bqK+2ZFaSiPF+RCuvJTZip1NhN3WxxuTzE+0n+zR5KIl//MpmIbUQVeFVVDqyWwNnrNWJzJzSnKoHuWTTYf0avwuSdKz5+kMeKkaVgvS1AFt5WOTPkj4TW/8z31a8xaXYRiasnKnnl+xXZL08N8a6qlbml7WBpR+vj6acnsb3dExQk5DeuyDTW7RdnrDmWYW7tLh7uYWNcyF5hM+/kU//lr01N331x/Us2fWAj1+c2P1L+amt5H1KmvGne3k52PT0o2HNOmzref9QGDmd78q7quC/5+Mu7WZBkbVKdZzehKbzaaX/nnxDoKL1h3Qe+sOymaTXu3fVrUqXf6HM3/0xM1NFFWvsk5l5+n+dxKUkZ13RdfD5Vm375gGvrFGaadz1bZ2RS0cerUqX+Y2Db3ahGv6gHby97Xp882H9cA7CaXy4fU7P+03/y54NPoqDbuuwWVfw8fHphf6tjr7QcL8BJYNlDHClZeioYV7+3H3UT32wSZJUkyXuhp6bf0rul5IoL/m3R2pGiEO/XokQ/fMW/+X/NlvPHhC/z7zfR16bb3LWiNxPjc0rqaxtxR0EJy8fJu+3u5e64wuh2EYmrxsmxk+/92tsUbd1PiygpWLr49Nk/u0NNf9jV26xdLOlWmnc7XrTFWgjRtUrlyG39jQrPI98E6i9v2pg+CKLcka/VFBN8th19XXA9df/h9Tf/S3JmHm+qK3ftynV878rP9o3o/79NzygnD9726Ndc819a7oOT1JYICvXr+rg6oFFd1BcPNvaRr38S+SpH/931WXPDXzQvx8fTRtQDuFBdu1O/WUnvjoZ6+qgruzVbuOatCba3UqO09X16+s+fdEKSTQv1jXurlFdc0e1EF2Px/Fb0/VvfPWKzOn5ILyhwm/aezSLZKk+69voIe7Niz2tfx8fRTXr42im4YpO8+pe99er/X7jpXUUHERhCsvxZor97X1ULqGzU8wO7ONu6VZsf64/bMaIYGad3ekgh1+Sth/vNhthz1VclqW2Ur8b02qaXT3piV27ZguddU/MuJMB8GNJbppa1lxOg2N//gXzT5TfRt/azMNv7H4//GWCj4hndCzmR64oSAQ/OfzbZpaxB/zZWHTmS6BdaqUU9UKdkvGUBSbzabn+7ZS6yI6CP64+6gefneDnIZ0e4daGtO9SYn8LujdNlxP/71gjVDcV7v01g9nK66L1h3QhE8KwsNDf2t4xe8BT1Q9xGH+kfzHDoLHM3J0/zsJZzZwr6YHbyi5701okF2vDWxn7lE454d9JXZtFO3r7Sm6e946nc7N13VXhWrukMjLWntblBsaV9NbMZEqF+CrVbuPasic4u+f9kefbjqkxz8s+GBwSOe6euLm4n3o9Uf+vgVTuK9tdGbD8bnr9PNvJ654rLg4wpWXcq258sZF+J7st+OZGjK34FO0qHqV9fJtrS9pnculuiosSLMHdVCAn4++3JqiCZ941zqh8zmdk6+hb69X6smChcoX2uSzOAo6CLbQ1fULpvbcM2+djmV4ztTL/DObUM7/ab9sNin2Hy11dwlVK2w2mx7v1lj/+r+C6W8vr9ypF1ZsL/P3nbneKqJimT7vpXD4+2r2Xe3NyvJD725Q4oHjBdsE5DvVrXmYJvdpWSLBymVw57pmA4eJn27V0g1J+nhjkkYv3iypYJ3dqDM/s7+i1hEV9fLtZzsIvrv2gB5ZtFFJJ06rTpVyevn2NiX6u1mS2tepbG74OnnZNq3dSyWhtCzffFj3zS8Iyjc1C9PsQe2L3DeyODo1qKL590QpyFHQQObON69sKv6XvyTr0UUb5TSkOzpGaPytJfOBq3Rm2cBdHRR5Zl32XW+u1bbDNL4qbTbjr/CX12VKT09XSEiI0tLSFBwcbPVwimXZ5sN6cEGiGlar4DH/AS3urxJPeQMbhjRl5Q79eiRDjcOC9P79nYo9PeFilm0+rOELE2UYBRtatq/jHmtQSsuSDUlauTVFlcsH6OPhXYrVwOJSHM/IUe/XftD+3zPVsW4ls+3t5bDi/fr5z4f1+ebD8vWx6aXbWqlP29JpXPDG//aY6wUGRNXWNZfZoOVKzPp+jzYdPKFJvZpf1gLwsrQlKU23zVyt07n58vWxKd9pqHODKpozpGOxGyZciGEYevrTrXrrx33mhw35TkN3Xl1bz/RqUaJhzlP9d+XOQlMnHf4+WvJgFzWtUTr/7TcMQyMXbdTHGw8pNMiu8bc2K9EPgiAdOJapF1Zsl9OQerauqSm3ty5ye4krtSUpTXe9uUbHM3PVtEawRtzYUJf7f6mjp7L1n8+2KSffqT5tw/XSba1L5f1wKjtPd725RhsOnFCV8gEad2szBfi5V33lfK/ax8embs2L3t+rLF1ONiBcFcEbwtW3O1I1ZO46q4eBItQIcWjxg51VI+TCLWCv1Fs/7DW7wf0V+PvatODeqxVZr/gNLC7F7tST6jP9R530sEXpfj42Te3fVt1bXl4Xuss1/6f9Gndm3YAVPh1xjVrWurTtDKywYsth3f9OoiSpda0QLRh69RVPVboQp9PQvz7YpCUbCva8+Wf7Wnqhb6sSr8p4KqfT0EPvbtDnZ9qk/7df61L78MElMydPfab/qB0eOL3Yk9zWvpae63vh7SWu1I7kkxr4xhodvcK9ELu3qK6p/duW6hYSaadzNWD2T/rFw7Zssfv5aMd/uls9DMLVlfKGcJWVm6/RH/2sQyeyin+Rsvxv71/kXVipvL/+3a1Jod3gS9P8n/brs02Hivft9aCfib+fTTGd65XZ5qfr9h3TtK93X9ZmrVay+/to2HX1S2Rx/qVYtvmw3vlpv/Lyy/ZN1KxmsCb0LLkpNaXlo4TftGbv7xrdvelldy0rjtx8p176cocCfH00MvoqKiV/cjonX88t36aG1SrorjKqeh48lqnnlm/XkZPeu0G5la5vHKoHrm9QJh8i7DlySi9+sUO/nyre1MDWESH6d7cmZVJJOpaRo2c/36aDx9xryxbjAn9w+Pv6aOHQq8twNEUjXF0hbwhXAAAAAK7c5WQD95pwCQAAAAAeinAFAAAAACWAcAUAAAAAJYBwBQAAAAAlwC3C1fTp01W3bl05HA5FRUVp7dq1Fzw/Li5OjRs3VmBgoCIiIvToo48qK+tsV7zY2Fh17NhRQUFBqlatmnr37q0dO3aU9ssAAAAA8BdmebhatGiRRo0apQkTJigxMVGtW7dWt27dlJqaWuT5Cxcu1OjRozVhwgRt27ZNb775phYtWqQnn3zSPOe7777T8OHD9dNPP2nlypXKzc3VTTfdpIyMjLJ6WQAAAAD+YixvxR4VFaWOHTtq2rRpkiSn06mIiAg99NBDGj169DnnjxgxQtu2bVN8fLx57F//+pfWrFmjVatWFfkcR44cUbVq1fTdd9/puuuuu+iYaMUOAAAAQPKgVuw5OTlKSEhQdHS0eczHx0fR0dFavXp1kY/p3LmzEhISzKmDe/bs0bJly9SjR4/zPk9aWpokqXLlykXen52drfT09EI3AAAAALgcflY++dGjR5Wfn6+wsLBCx8PCwrR9+/YiHzNgwAAdPXpU11xzjQzDUF5enu6///5C0wL/yOl0auTIkerSpYtatGhR5DmxsbF6+umnr+zFAAAAAPhLs3zN1eX69ttvNXnyZL322mtKTEzU4sWL9fnnn+uZZ54p8vzhw4dry5Yteu+99857zTFjxigtLc28HTx4sLSGDwAAAMBLWVq5qlq1qnx9fZWSklLoeEpKiqpXr17kY8aNG6e77rpL9957rySpZcuWysjI0LBhw/TUU0/Jx+dsXhwxYoQ+++wzff/996pVq9Z5x2G322W320vgFQEAAAD4q7K0chUQEKD27dsXak7hdDoVHx+vTp06FfmYzMzMQgFKknx9fSVJrt4chmFoxIgRWrJkib7++mvVq1evlF4BAAAAABSwtHIlSaNGjdLgwYPVoUMHRUZGKi4uThkZGYqJiZEkDRo0SOHh4YqNjZUk9ezZU1OmTFHbtm0VFRWl3bt3a9y4cerZs6cZsoYPH66FCxfq448/VlBQkJKTkyVJISEhCgwMtOaFAgAAAPBqloerfv366ciRIxo/frySk5PVpk0brVixwmxyceDAgUKVqrFjx8pms2ns2LFKSkpSaGioevbsqWeffdY8Z8aMGZKkG264odBzzZ07V0OGDCn11wQAAADgr8fyfa7cEftcAQAAAJA8aJ8rAAAAAPAWhCsAAAAAKAGEKwAAAAAoAZY3tHBHrmVo6enpFo8EAAAAgJVcmeBSWlUQropw8uRJSVJERITFIwEAAADgDk6ePKmQkJALnkO3wCI4nU4dOnRIQUFBstlslo4lPT1dEREROnjwIJ0Lcdl4/+BK8P7BleD9gyvB+wfFVRrvHcMwdPLkSdWsWbPQFlFFoXJVBB8fH9WqVcvqYRQSHBzMLxcUG+8fXAneP7gSvH9wJXj/oLhK+r1zsYqVCw0tAAAAAKAEEK4AAAAAoAQQrtyc3W7XhAkTZLfbrR4KPBDvH1wJ3j+4Erx/cCV4/6C4rH7v0NACAAAAAEoAlSsAAAAAKAGEKwAAAAAoAYQrAAAAACgBhCsAAAAAKAGEKzc3ffp01a1bVw6HQ1FRUVq7dq3VQ4Ib+v7779WzZ0/VrFlTNptNS5cuLXS/YRgaP368atSoocDAQEVHR2vXrl3WDBZuJTY2Vh07dlRQUJCqVaum3r17a8eOHYXOycrK0vDhw1WlShVVqFBBffv2VUpKikUjhjuZMWOGWrVqZW7W2alTJy1fvty8n/cOLtVzzz0nm82mkSNHmsd4/+BCJk6cKJvNVujWpEkT836r3j+EKze2aNEijRo1ShMmTFBiYqJat26tbt26KTU11eqhwc1kZGSodevWmj59epH3v/DCC3r11Vc1c+ZMrVmzRuXLl1e3bt2UlZVVxiOFu/nuu+80fPhw/fTTT1q5cqVyc3N10003KSMjwzzn0Ucf1aeffqoPPvhA3333nQ4dOqR//OMfFo4a7qJWrVp67rnnlJCQoPXr1+tvf/ubevXqpV9++UUS7x1cmnXr1mnWrFlq1apVoeO8f3AxzZs31+HDh83bqlWrzPsse/8YcFuRkZHG8OHDza/z8/ONmjVrGrGxsRaOCu5OkrFkyRLza6fTaVSvXt148cUXzWMnTpww7Ha78e6771owQriz1NRUQ5Lx3XffGYZR8F7x9/c3PvjgA/Ocbdu2GZKM1atXWzVMuLFKlSoZb7zxBu8dXJKTJ08ajRo1MlauXGlcf/31xiOPPGIYBr97cHETJkwwWrduXeR9Vr5/qFy5qZycHCUkJCg6Oto85uPjo+joaK1evdrCkcHT7N27V8nJyYXeSyEhIYqKiuK9hHOkpaVJkipXrixJSkhIUG5ubqH3T5MmTVS7dm3ePygkPz9f7733njIyMtSpUyfeO7gkw4cP1y233FLofSLxuweXZteuXapZs6bq16+vgQMH6sCBA5Ksff/4lerVUWxHjx5Vfn6+wsLCCh0PCwvT9u3bLRoVPFFycrIkFflect0HSJLT6dTIkSPVpUsXtWjRQlLB+ycgIEAVK1YsdC7vH7hs3rxZnTp1UlZWlipUqKAlS5aoWbNm2rhxI+8dXNB7772nxMRErVu37pz7+N2Di4mKitJbb72lxo0b6/Dhw3r66ad17bXXasuWLZa+fwhXAABJBZ8gb9mypdCcdeBiGjdurI0bNyotLU0ffvihBg8erO+++87qYcHNHTx4UI888ohWrlwph8Nh9XDggbp3727+u1WrVoqKilKdOnX0/vvvKzAw0LJxMS3QTVWtWlW+vr7ndDVJSUlR9erVLRoVPJHr/cJ7CRcyYsQIffbZZ/rmm29Uq1Yt83j16tWVk5OjEydOFDqf9w9cAgIC1LBhQ7Vv316xsbFq3bq1XnnlFd47uKCEhASlpqaqXbt28vPzk5+fn7777ju9+uqr8vPzU1hYGO8fXJaKFSvqqquu0u7duy39/UO4clMBAQFq37694uPjzWNOp1Px8fHq1KmThSODp6lXr56qV69e6L2Unp6uNWvW8F6CDMPQiBEjtGTJEn399deqV69eofvbt28vf3//Qu+fHTt26MCBA7x/UCSn06ns7GzeO7igrl27avPmzdq4caN569ChgwYOHGj+m/cPLsepU6f066+/qkaNGpb+/mFaoBsbNWqUBg8erA4dOigyMlJxcXHKyMhQTEyM1UODmzl16pR2795tfr13715t3LhRlStXVu3atTVy5Ej95z//UaNGjVSvXj2NGzdONWvWVO/eva0bNNzC8OHDtXDhQn388ccKCgoy56KHhIQoMDBQISEhuueeezRq1ChVrlxZwcHBeuihh9SpUyddffXVFo8eVhszZoy6d++u2rVr6+TJk1q4cKG+/fZbffHFF7x3cEFBQUHm2k6X8uXLq0qVKuZx3j+4kMcee0w9e/ZUnTp1dOjQIU2YMEG+vr7q37+/tb9/SrUXIa7Y1KlTjdq1axsBAQFGZGSk8dNPP1k9JLihb775xpB0zm3w4MGGYRS0Yx83bpwRFhZm2O12o2vXrsaOHTusHTTcQlHvG0nG3LlzzXNOnz5tPPjgg0alSpWMcuXKGX369DEOHz5s3aDhNu6++26jTp06RkBAgBEaGmp07drV+PLLL837ee/gcvyxFbth8P7BhfXr18+oUaOGERAQYISHhxv9+vUzdu/ebd5v1fvHZhiGUbrxDQAAAAC8H2uuAAAAAKAEEK4AAAAAoAQQrgAAAACgBBCuAAAAAKAEEK4AAAAAoAQQrgAAAACgBBCuAAAAAKAEEK4AAAAAoAQQrgAAKGE2m01Lly61ehgAgDJGuAIAeJUhQ4bIZrOdc7v55putHhoAwMv5WT0AAABK2s0336y5c+cWOma32y0aDQDgr4LKFQDA69jtdlWvXr3QrVKlSpIKpuzNmDFD3bt3V2BgoOrXr68PP/yw0OM3b96sv/3tbwoMDFSVKlU0bNgwnTp1qtA5c+bMUfPmzWW321WjRg2NGDGi0P1Hjx5Vnz59VK5cOTVq1EiffPJJ6b5oAIDlCFcAgL+ccePGqW/fvtq0aZMGDhyoO+64Q9u2bZMkZWRkqFu3bqpUqZLWrVunDz74QF999VWh8DRjxgwNHz5cw4YN0+bNm/XJJ5+oYcOGhZ7j6aef1u23366ff/5ZPXr00MCBA3Xs2LEyfZ0AgLJlMwzDsHoQAACUlCFDhuidd96Rw+EodPzJJ5/Uk08+KZvNpvvvv18zZsww77v66qvVrl07vfbaa5o9e7aeeOIJHTx4UOXLl5ckLVu2TD179tShQ4cUFham8PBwxcTE6D//+U+RY7DZbBo7dqyeeeYZSQWBrUKFClq+fDlrvwDAi7HmCgDgdW688cZC4UmSKleubP67U6dOhe7r1KmTNm7cKEnatm2bWrdubQYrSerSpYucTqd27Nghm82mQ4cOqWvXrhccQ6tWrcx/ly9fXsHBwUpNTS3uSwIAeADCFQDA65QvX/6caXolJTAw8JLO8/f3L/S1zWaT0+ksjSEBANwEa64AAH85P/300zlfN23aVJLUtGlTbdq0SRkZGeb9P/zwg3x8fNS4cWMFBQWpbt26io+PL9MxAwDcH5UrAIDXyc7OVnJycqFjfn5+qlq1qiTpgw8+UIcOHXTNNddowYIFWrt2rd58801J0sCBAzVhwgQNHjxYEydO1JEjR/TQQw/prrvuUlhYmCRp4sSJuv/++1WtWjV1795dJ0+e1A8//KCHHnqobF8oAMCtEK4AAF5nxYoVqlGjRqFjjRs31vbt2yUVdPJ777339OCDD6pGjRp699131axZM0lSuXLl9MUXX+iRRx5Rx44dVa5cOfXt21dTpkwxrzV48GBlZWXpv//9rx577DFVrVpV//znP8vuBQIA3BLdAgEAfyk2m01LlixR7969rR4KAMDLsOYKAAAAAEoA4QoAAAAASgBrrgAAfynMhgcAlBYqVwAAAABQAghXAAAAAFACCFcAAAAAUAIIVwAAAABQAghXAAAAAFACCFcAAAAAUAIIVwAAAABQAghXAAAAAFAC/h+1EA13GxPe5gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Task-wise loss (optional): assumes model outputs task-wise loss components\n",
    "# Uncomment if you have task-wise history saved in history\n",
    "# for i in range(num_tasks):\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.plot(history.history[f'task_{i}_loss'], label=f'Task {i} Loss')\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.ylabel(f'Task {i} Loss')\n",
    "#     plt.title(f'Task {i} Loss over Epochs')\n",
    "#     plt.legend()\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test data from Tox21 dataset\n",
    "x_train = pd.read_csv('data/tox21_dense_train.csv')\n",
    "y_train = pd.read_csv('data/tox21_labels_train.csv')\n",
    "\n",
    "x_test = pd.read_csv('data/tox21_dense_test.csv')\n",
    "y_test = pd.read_csv('data/tox21_labels_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>AW</th>\n",
       "      <th>AWeight</th>\n",
       "      <th>Arto</th>\n",
       "      <th>BertzCT</th>\n",
       "      <th>Chi0</th>\n",
       "      <th>Chi1</th>\n",
       "      <th>Chi10</th>\n",
       "      <th>Chi2</th>\n",
       "      <th>Chi3</th>\n",
       "      <th>...</th>\n",
       "      <th>W3D</th>\n",
       "      <th>W3DH</th>\n",
       "      <th>WNSA1</th>\n",
       "      <th>WNSA2</th>\n",
       "      <th>WNSA3</th>\n",
       "      <th>WPSA1</th>\n",
       "      <th>WPSA2</th>\n",
       "      <th>WPSA3</th>\n",
       "      <th>grav</th>\n",
       "      <th>rygr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NCGC00178831-03</td>\n",
       "      <td>5.436720e+07</td>\n",
       "      <td>13.053</td>\n",
       "      <td>2.176</td>\n",
       "      <td>3.194</td>\n",
       "      <td>23.112</td>\n",
       "      <td>15.868</td>\n",
       "      <td>1.496</td>\n",
       "      <td>15.127</td>\n",
       "      <td>12.592</td>\n",
       "      <td>...</td>\n",
       "      <td>2687.469</td>\n",
       "      <td>9241.018</td>\n",
       "      <td>115.371</td>\n",
       "      <td>-915.496</td>\n",
       "      <td>-39.983</td>\n",
       "      <td>290.078</td>\n",
       "      <td>2301.941</td>\n",
       "      <td>59.492</td>\n",
       "      <td>88.147</td>\n",
       "      <td>3.708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NCGC00166114-03</td>\n",
       "      <td>1.268818e+07</td>\n",
       "      <td>22.123</td>\n",
       "      <td>2.065</td>\n",
       "      <td>3.137</td>\n",
       "      <td>21.033</td>\n",
       "      <td>13.718</td>\n",
       "      <td>1.937</td>\n",
       "      <td>13.187</td>\n",
       "      <td>11.951</td>\n",
       "      <td>...</td>\n",
       "      <td>2184.384</td>\n",
       "      <td>3234.199</td>\n",
       "      <td>194.740</td>\n",
       "      <td>-1029.609</td>\n",
       "      <td>-34.205</td>\n",
       "      <td>235.360</td>\n",
       "      <td>1244.323</td>\n",
       "      <td>82.906</td>\n",
       "      <td>134.852</td>\n",
       "      <td>4.131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NCGC00263563-01</td>\n",
       "      <td>3.076932e+06</td>\n",
       "      <td>13.085</td>\n",
       "      <td>2.154</td>\n",
       "      <td>3.207</td>\n",
       "      <td>46.896</td>\n",
       "      <td>29.958</td>\n",
       "      <td>3.806</td>\n",
       "      <td>30.105</td>\n",
       "      <td>25.569</td>\n",
       "      <td>...</td>\n",
       "      <td>13803.524</td>\n",
       "      <td>76582.899</td>\n",
       "      <td>238.004</td>\n",
       "      <td>-4358.946</td>\n",
       "      <td>-106.537</td>\n",
       "      <td>868.685</td>\n",
       "      <td>15909.444</td>\n",
       "      <td>135.335</td>\n",
       "      <td>216.852</td>\n",
       "      <td>5.075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NCGC00013058-02</td>\n",
       "      <td>7.168569e+07</td>\n",
       "      <td>12.832</td>\n",
       "      <td>2.029</td>\n",
       "      <td>3.380</td>\n",
       "      <td>51.086</td>\n",
       "      <td>32.045</td>\n",
       "      <td>1.806</td>\n",
       "      <td>29.090</td>\n",
       "      <td>21.603</td>\n",
       "      <td>...</td>\n",
       "      <td>13807.345</td>\n",
       "      <td>50498.175</td>\n",
       "      <td>226.312</td>\n",
       "      <td>-2785.555</td>\n",
       "      <td>-61.923</td>\n",
       "      <td>763.288</td>\n",
       "      <td>9394.859</td>\n",
       "      <td>125.509</td>\n",
       "      <td>238.265</td>\n",
       "      <td>4.640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NCGC00167516-01</td>\n",
       "      <td>7.989702e+06</td>\n",
       "      <td>12.936</td>\n",
       "      <td>2.124</td>\n",
       "      <td>3.573</td>\n",
       "      <td>70.295</td>\n",
       "      <td>46.402</td>\n",
       "      <td>3.604</td>\n",
       "      <td>42.132</td>\n",
       "      <td>32.570</td>\n",
       "      <td>...</td>\n",
       "      <td>43231.286</td>\n",
       "      <td>163659.229</td>\n",
       "      <td>850.869</td>\n",
       "      <td>-21136.699</td>\n",
       "      <td>-367.122</td>\n",
       "      <td>1798.703</td>\n",
       "      <td>44681.209</td>\n",
       "      <td>362.168</td>\n",
       "      <td>317.901</td>\n",
       "      <td>7.845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 802 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0            AW  AWeight   Arto  BertzCT    Chi0    Chi1  \\\n",
       "0  NCGC00178831-03  5.436720e+07   13.053  2.176    3.194  23.112  15.868   \n",
       "1  NCGC00166114-03  1.268818e+07   22.123  2.065    3.137  21.033  13.718   \n",
       "2  NCGC00263563-01  3.076932e+06   13.085  2.154    3.207  46.896  29.958   \n",
       "3  NCGC00013058-02  7.168569e+07   12.832  2.029    3.380  51.086  32.045   \n",
       "4  NCGC00167516-01  7.989702e+06   12.936  2.124    3.573  70.295  46.402   \n",
       "\n",
       "   Chi10    Chi2    Chi3  ...        W3D        W3DH    WNSA1      WNSA2  \\\n",
       "0  1.496  15.127  12.592  ...   2687.469    9241.018  115.371   -915.496   \n",
       "1  1.937  13.187  11.951  ...   2184.384    3234.199  194.740  -1029.609   \n",
       "2  3.806  30.105  25.569  ...  13803.524   76582.899  238.004  -4358.946   \n",
       "3  1.806  29.090  21.603  ...  13807.345   50498.175  226.312  -2785.555   \n",
       "4  3.604  42.132  32.570  ...  43231.286  163659.229  850.869 -21136.699   \n",
       "\n",
       "     WNSA3     WPSA1      WPSA2    WPSA3     grav   rygr  \n",
       "0  -39.983   290.078   2301.941   59.492   88.147  3.708  \n",
       "1  -34.205   235.360   1244.323   82.906  134.852  4.131  \n",
       "2 -106.537   868.685  15909.444  135.335  216.852  5.075  \n",
       "3  -61.923   763.288   9394.859  125.509  238.265  4.640  \n",
       "4 -367.122  1798.703  44681.209  362.168  317.901  7.845  \n",
       "\n",
       "[5 rows x 802 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n_samples, d_features): (12060, 802)\n",
      "Index(['Unnamed: 0', 'AW', 'AWeight', 'Arto', 'BertzCT', 'Chi0', 'Chi1',\n",
      "       'Chi10', 'Chi2', 'Chi3',\n",
      "       ...\n",
      "       'W3D', 'W3DH', 'WNSA1', 'WNSA2', 'WNSA3', 'WPSA1', 'WPSA2', 'WPSA3',\n",
      "       'grav', 'rygr'],\n",
      "      dtype='object', length=802)\n"
     ]
    }
   ],
   "source": [
    "# Check out the data\n",
    "x_train.head()\n",
    "\n",
    "print(f\"(n_samples, d_features): {x_train.shape}\")\n",
    "print(f\"{x_train.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n_samples, m_tasks): (12060, 13)\n",
      "Index(['Unnamed: 0', 'NR.AhR', 'NR.AR', 'NR.AR.LBD', 'NR.Aromatase', 'NR.ER',\n",
      "       'NR.ER.LBD', 'NR.PPAR.gamma', 'SR.ARE', 'SR.ATAD5', 'SR.HSE', 'SR.MMP',\n",
      "       'SR.p53'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "y_train.head()\n",
    "\n",
    "print(f\"(n_samples, m_tasks): {y_train.shape}\")\n",
    "print(f\"{y_train.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in x_train: 0\n",
      "Missing values in y_train: 43238\n",
      "Missing values in x_test: 0\n",
      "Missing values in y_test: 791\n"
     ]
    }
   ],
   "source": [
    "# Check missing values\n",
    "print(f\"Missing values in x_train: {x_train.isnull().sum().sum()}\")\n",
    "print(f\"Missing values in y_train: {y_train.isnull().sum().sum()}\")\n",
    "\n",
    "# Check for missing values in test data\n",
    "print(f\"Missing values in x_test: {x_test.isnull().sum().sum()}\")\n",
    "print(f\"Missing values in y_test: {y_test.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AW</th>\n",
       "      <th>AWeight</th>\n",
       "      <th>Arto</th>\n",
       "      <th>BertzCT</th>\n",
       "      <th>Chi0</th>\n",
       "      <th>Chi1</th>\n",
       "      <th>Chi10</th>\n",
       "      <th>Chi2</th>\n",
       "      <th>Chi3</th>\n",
       "      <th>Chi3c</th>\n",
       "      <th>...</th>\n",
       "      <th>W3D</th>\n",
       "      <th>W3DH</th>\n",
       "      <th>WNSA1</th>\n",
       "      <th>WNSA2</th>\n",
       "      <th>WNSA3</th>\n",
       "      <th>WPSA1</th>\n",
       "      <th>WPSA2</th>\n",
       "      <th>WPSA3</th>\n",
       "      <th>grav</th>\n",
       "      <th>rygr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.206000e+04</td>\n",
       "      <td>12060.000000</td>\n",
       "      <td>12060.000000</td>\n",
       "      <td>12060.000000</td>\n",
       "      <td>12060.000000</td>\n",
       "      <td>12060.00000</td>\n",
       "      <td>12060.000000</td>\n",
       "      <td>12060.000000</td>\n",
       "      <td>12060.000000</td>\n",
       "      <td>12060.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>12060.000000</td>\n",
       "      <td>12060.000000</td>\n",
       "      <td>12060.000000</td>\n",
       "      <td>12060.000000</td>\n",
       "      <td>12060.000000</td>\n",
       "      <td>12060.000000</td>\n",
       "      <td>12060.000000</td>\n",
       "      <td>12060.000000</td>\n",
       "      <td>12060.000000</td>\n",
       "      <td>12060.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.219191e+06</td>\n",
       "      <td>14.510179</td>\n",
       "      <td>2.011927</td>\n",
       "      <td>2.546282</td>\n",
       "      <td>14.001534</td>\n",
       "      <td>9.01234</td>\n",
       "      <td>0.522415</td>\n",
       "      <td>8.201283</td>\n",
       "      <td>6.307590</td>\n",
       "      <td>1.600608</td>\n",
       "      <td>...</td>\n",
       "      <td>1248.363297</td>\n",
       "      <td>5293.483884</td>\n",
       "      <td>85.805640</td>\n",
       "      <td>-555.791432</td>\n",
       "      <td>-30.399123</td>\n",
       "      <td>186.128018</td>\n",
       "      <td>1238.731087</td>\n",
       "      <td>34.164007</td>\n",
       "      <td>123.783499</td>\n",
       "      <td>3.219006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.175669e+07</td>\n",
       "      <td>4.787198</td>\n",
       "      <td>0.195923</td>\n",
       "      <td>0.435071</td>\n",
       "      <td>8.091058</td>\n",
       "      <td>5.32414</td>\n",
       "      <td>0.826746</td>\n",
       "      <td>5.153103</td>\n",
       "      <td>4.387759</td>\n",
       "      <td>1.331670</td>\n",
       "      <td>...</td>\n",
       "      <td>3180.775783</td>\n",
       "      <td>12823.738015</td>\n",
       "      <td>72.904885</td>\n",
       "      <td>1918.257248</td>\n",
       "      <td>36.252950</td>\n",
       "      <td>165.551810</td>\n",
       "      <td>3232.420359</td>\n",
       "      <td>33.984633</td>\n",
       "      <td>3842.918796</td>\n",
       "      <td>0.994193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>11.366000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.301000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.206000</td>\n",
       "      <td>1.688000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-120941.468000</td>\n",
       "      <td>-1235.074000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.594000</td>\n",
       "      <td>0.162000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.077000e+00</td>\n",
       "      <td>12.723000</td>\n",
       "      <td>1.941000</td>\n",
       "      <td>2.323000</td>\n",
       "      <td>8.552000</td>\n",
       "      <td>5.34300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.767000</td>\n",
       "      <td>3.288000</td>\n",
       "      <td>0.731000</td>\n",
       "      <td>...</td>\n",
       "      <td>203.316750</td>\n",
       "      <td>915.805250</td>\n",
       "      <td>43.300500</td>\n",
       "      <td>-544.346750</td>\n",
       "      <td>-36.667000</td>\n",
       "      <td>89.507000</td>\n",
       "      <td>236.484250</td>\n",
       "      <td>16.736500</td>\n",
       "      <td>21.649000</td>\n",
       "      <td>2.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.199000e+00</td>\n",
       "      <td>13.342000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.631000</td>\n",
       "      <td>12.466000</td>\n",
       "      <td>8.04100</td>\n",
       "      <td>0.217000</td>\n",
       "      <td>7.223000</td>\n",
       "      <td>5.439000</td>\n",
       "      <td>1.280000</td>\n",
       "      <td>...</td>\n",
       "      <td>550.826000</td>\n",
       "      <td>2236.311500</td>\n",
       "      <td>69.967500</td>\n",
       "      <td>-261.439000</td>\n",
       "      <td>-22.159000</td>\n",
       "      <td>146.220000</td>\n",
       "      <td>561.240000</td>\n",
       "      <td>26.911500</td>\n",
       "      <td>36.319500</td>\n",
       "      <td>3.142000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.609000e+00</td>\n",
       "      <td>14.583000</td>\n",
       "      <td>2.138000</td>\n",
       "      <td>2.853000</td>\n",
       "      <td>17.225250</td>\n",
       "      <td>11.30725</td>\n",
       "      <td>0.706000</td>\n",
       "      <td>10.426500</td>\n",
       "      <td>8.433500</td>\n",
       "      <td>2.070000</td>\n",
       "      <td>...</td>\n",
       "      <td>1290.369250</td>\n",
       "      <td>5515.172000</td>\n",
       "      <td>107.567750</td>\n",
       "      <td>-112.989250</td>\n",
       "      <td>-13.248500</td>\n",
       "      <td>234.616500</td>\n",
       "      <td>1259.678750</td>\n",
       "      <td>41.569000</td>\n",
       "      <td>57.437250</td>\n",
       "      <td>3.746000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000e+08</td>\n",
       "      <td>151.466000</td>\n",
       "      <td>2.667000</td>\n",
       "      <td>3.776000</td>\n",
       "      <td>94.695000</td>\n",
       "      <td>63.00600</td>\n",
       "      <td>12.188000</td>\n",
       "      <td>59.172000</td>\n",
       "      <td>50.798000</td>\n",
       "      <td>19.852000</td>\n",
       "      <td>...</td>\n",
       "      <td>93581.972000</td>\n",
       "      <td>355470.488000</td>\n",
       "      <td>1366.650000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3353.749000</td>\n",
       "      <td>101675.928000</td>\n",
       "      <td>1391.785000</td>\n",
       "      <td>397154.560000</td>\n",
       "      <td>15.511000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 801 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 AW       AWeight          Arto       BertzCT          Chi0  \\\n",
       "count  1.206000e+04  12060.000000  12060.000000  12060.000000  12060.000000   \n",
       "mean   4.219191e+06     14.510179      2.011927      2.546282     14.001534   \n",
       "std    1.175669e+07      4.787198      0.195923      0.435071      8.091058   \n",
       "min    1.000000e+00     11.366000      0.000000      0.301000      0.000000   \n",
       "25%    3.077000e+00     12.723000      1.941000      2.323000      8.552000   \n",
       "50%    4.199000e+00     13.342000      2.000000      2.631000     12.466000   \n",
       "75%    6.609000e+00     14.583000      2.138000      2.853000     17.225250   \n",
       "max    1.000000e+08    151.466000      2.667000      3.776000     94.695000   \n",
       "\n",
       "              Chi1         Chi10          Chi2          Chi3         Chi3c  \\\n",
       "count  12060.00000  12060.000000  12060.000000  12060.000000  12060.000000   \n",
       "mean       9.01234      0.522415      8.201283      6.307590      1.600608   \n",
       "std        5.32414      0.826746      5.153103      4.387759      1.331670   \n",
       "min        0.00000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        5.34300      0.000000      4.767000      3.288000      0.731000   \n",
       "50%        8.04100      0.217000      7.223000      5.439000      1.280000   \n",
       "75%       11.30725      0.706000     10.426500      8.433500      2.070000   \n",
       "max       63.00600     12.188000     59.172000     50.798000     19.852000   \n",
       "\n",
       "       ...           W3D           W3DH         WNSA1          WNSA2  \\\n",
       "count  ...  12060.000000   12060.000000  12060.000000   12060.000000   \n",
       "mean   ...   1248.363297    5293.483884     85.805640    -555.791432   \n",
       "std    ...   3180.775783   12823.738015     72.904885    1918.257248   \n",
       "min    ...      1.206000       1.688000      0.000000 -120941.468000   \n",
       "25%    ...    203.316750     915.805250     43.300500    -544.346750   \n",
       "50%    ...    550.826000    2236.311500     69.967500    -261.439000   \n",
       "75%    ...   1290.369250    5515.172000    107.567750    -112.989250   \n",
       "max    ...  93581.972000  355470.488000   1366.650000       0.000000   \n",
       "\n",
       "              WNSA3         WPSA1          WPSA2         WPSA3           grav  \\\n",
       "count  12060.000000  12060.000000   12060.000000  12060.000000   12060.000000   \n",
       "mean     -30.399123    186.128018    1238.731087     34.164007     123.783499   \n",
       "std       36.252950    165.551810    3232.420359     33.984633    3842.918796   \n",
       "min    -1235.074000      0.000000       0.000000      0.000000       1.594000   \n",
       "25%      -36.667000     89.507000     236.484250     16.736500      21.649000   \n",
       "50%      -22.159000    146.220000     561.240000     26.911500      36.319500   \n",
       "75%      -13.248500    234.616500    1259.678750     41.569000      57.437250   \n",
       "max        0.000000   3353.749000  101675.928000   1391.785000  397154.560000   \n",
       "\n",
       "               rygr  \n",
       "count  12060.000000  \n",
       "mean       3.219006  \n",
       "std        0.994193  \n",
       "min        0.162000  \n",
       "25%        2.530000  \n",
       "50%        3.142000  \n",
       "75%        3.746000  \n",
       "max       15.511000  \n",
       "\n",
       "[8 rows x 801 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 AW       AWeight          Arto       BertzCT          Chi0  \\\n",
      "count  1.206000e+04  12060.000000  12060.000000  12060.000000  12060.000000   \n",
      "mean   4.219191e+06     14.510179      2.011927      2.546282     14.001534   \n",
      "std    1.175669e+07      4.787198      0.195923      0.435071      8.091058   \n",
      "min    1.000000e+00     11.366000      0.000000      0.301000      0.000000   \n",
      "25%    3.077000e+00     12.723000      1.941000      2.323000      8.552000   \n",
      "50%    4.199000e+00     13.342000      2.000000      2.631000     12.466000   \n",
      "75%    6.609000e+00     14.583000      2.138000      2.853000     17.225250   \n",
      "max    1.000000e+08    151.466000      2.667000      3.776000     94.695000   \n",
      "\n",
      "              Chi1         Chi10          Chi2          Chi3         Chi3c  \\\n",
      "count  12060.00000  12060.000000  12060.000000  12060.000000  12060.000000   \n",
      "mean       9.01234      0.522415      8.201283      6.307590      1.600608   \n",
      "std        5.32414      0.826746      5.153103      4.387759      1.331670   \n",
      "min        0.00000      0.000000      0.000000      0.000000      0.000000   \n",
      "25%        5.34300      0.000000      4.767000      3.288000      0.731000   \n",
      "50%        8.04100      0.217000      7.223000      5.439000      1.280000   \n",
      "75%       11.30725      0.706000     10.426500      8.433500      2.070000   \n",
      "max       63.00600     12.188000     59.172000     50.798000     19.852000   \n",
      "\n",
      "       ...           W3D           W3DH         WNSA1          WNSA2  \\\n",
      "count  ...  12060.000000   12060.000000  12060.000000   12060.000000   \n",
      "mean   ...   1248.363297    5293.483884     85.805640    -555.791432   \n",
      "std    ...   3180.775783   12823.738015     72.904885    1918.257248   \n",
      "min    ...      1.206000       1.688000      0.000000 -120941.468000   \n",
      "25%    ...    203.316750     915.805250     43.300500    -544.346750   \n",
      "50%    ...    550.826000    2236.311500     69.967500    -261.439000   \n",
      "75%    ...   1290.369250    5515.172000    107.567750    -112.989250   \n",
      "max    ...  93581.972000  355470.488000   1366.650000       0.000000   \n",
      "\n",
      "              WNSA3         WPSA1          WPSA2         WPSA3           grav  \\\n",
      "count  12060.000000  12060.000000   12060.000000  12060.000000   12060.000000   \n",
      "mean     -30.399123    186.128018    1238.731087     34.164007     123.783499   \n",
      "std       36.252950    165.551810    3232.420359     33.984633    3842.918796   \n",
      "min    -1235.074000      0.000000       0.000000      0.000000       1.594000   \n",
      "25%      -36.667000     89.507000     236.484250     16.736500      21.649000   \n",
      "50%      -22.159000    146.220000     561.240000     26.911500      36.319500   \n",
      "75%      -13.248500    234.616500    1259.678750     41.569000      57.437250   \n",
      "max        0.000000   3353.749000  101675.928000   1391.785000  397154.560000   \n",
      "\n",
      "               rygr  \n",
      "count  12060.000000  \n",
      "mean       3.219006  \n",
      "std        0.994193  \n",
      "min        0.162000  \n",
      "25%        2.530000  \n",
      "50%        3.142000  \n",
      "75%        3.746000  \n",
      "max       15.511000  \n",
      "\n",
      "[8 rows x 801 columns]\n"
     ]
    }
   ],
   "source": [
    "# Standard Deviation\n",
    "std_scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data\n",
    "std_scaler.fit_transform(x_train.iloc[:, 1:])\n",
    "\n",
    "print(x_train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>AW</th>\n",
       "      <th>AWeight</th>\n",
       "      <th>Arto</th>\n",
       "      <th>BertzCT</th>\n",
       "      <th>Chi0</th>\n",
       "      <th>Chi1</th>\n",
       "      <th>Chi10</th>\n",
       "      <th>Chi2</th>\n",
       "      <th>Chi3</th>\n",
       "      <th>...</th>\n",
       "      <th>W3D</th>\n",
       "      <th>W3DH</th>\n",
       "      <th>WNSA1</th>\n",
       "      <th>WNSA2</th>\n",
       "      <th>WNSA3</th>\n",
       "      <th>WPSA1</th>\n",
       "      <th>WPSA2</th>\n",
       "      <th>WPSA3</th>\n",
       "      <th>grav</th>\n",
       "      <th>rygr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NCGC00178831-03</td>\n",
       "      <td>5.436720e+07</td>\n",
       "      <td>13.053</td>\n",
       "      <td>2.176</td>\n",
       "      <td>3.194</td>\n",
       "      <td>23.112</td>\n",
       "      <td>15.868</td>\n",
       "      <td>1.496</td>\n",
       "      <td>15.127</td>\n",
       "      <td>12.592</td>\n",
       "      <td>...</td>\n",
       "      <td>2687.469</td>\n",
       "      <td>9241.018</td>\n",
       "      <td>115.371</td>\n",
       "      <td>-915.496</td>\n",
       "      <td>-39.983</td>\n",
       "      <td>290.078</td>\n",
       "      <td>2301.941</td>\n",
       "      <td>59.492</td>\n",
       "      <td>88.147</td>\n",
       "      <td>3.708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NCGC00166114-03</td>\n",
       "      <td>1.268818e+07</td>\n",
       "      <td>22.123</td>\n",
       "      <td>2.065</td>\n",
       "      <td>3.137</td>\n",
       "      <td>21.033</td>\n",
       "      <td>13.718</td>\n",
       "      <td>1.937</td>\n",
       "      <td>13.187</td>\n",
       "      <td>11.951</td>\n",
       "      <td>...</td>\n",
       "      <td>2184.384</td>\n",
       "      <td>3234.199</td>\n",
       "      <td>194.740</td>\n",
       "      <td>-1029.609</td>\n",
       "      <td>-34.205</td>\n",
       "      <td>235.360</td>\n",
       "      <td>1244.323</td>\n",
       "      <td>82.906</td>\n",
       "      <td>134.852</td>\n",
       "      <td>4.131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NCGC00263563-01</td>\n",
       "      <td>3.076932e+06</td>\n",
       "      <td>13.085</td>\n",
       "      <td>2.154</td>\n",
       "      <td>3.207</td>\n",
       "      <td>46.896</td>\n",
       "      <td>29.958</td>\n",
       "      <td>3.806</td>\n",
       "      <td>30.105</td>\n",
       "      <td>25.569</td>\n",
       "      <td>...</td>\n",
       "      <td>13803.524</td>\n",
       "      <td>76582.899</td>\n",
       "      <td>238.004</td>\n",
       "      <td>-4358.946</td>\n",
       "      <td>-106.537</td>\n",
       "      <td>868.685</td>\n",
       "      <td>15909.444</td>\n",
       "      <td>135.335</td>\n",
       "      <td>216.852</td>\n",
       "      <td>5.075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NCGC00013058-02</td>\n",
       "      <td>7.168569e+07</td>\n",
       "      <td>12.832</td>\n",
       "      <td>2.029</td>\n",
       "      <td>3.380</td>\n",
       "      <td>51.086</td>\n",
       "      <td>32.045</td>\n",
       "      <td>1.806</td>\n",
       "      <td>29.090</td>\n",
       "      <td>21.603</td>\n",
       "      <td>...</td>\n",
       "      <td>13807.345</td>\n",
       "      <td>50498.175</td>\n",
       "      <td>226.312</td>\n",
       "      <td>-2785.555</td>\n",
       "      <td>-61.923</td>\n",
       "      <td>763.288</td>\n",
       "      <td>9394.859</td>\n",
       "      <td>125.509</td>\n",
       "      <td>238.265</td>\n",
       "      <td>4.640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NCGC00167516-01</td>\n",
       "      <td>7.989702e+06</td>\n",
       "      <td>12.936</td>\n",
       "      <td>2.124</td>\n",
       "      <td>3.573</td>\n",
       "      <td>70.295</td>\n",
       "      <td>46.402</td>\n",
       "      <td>3.604</td>\n",
       "      <td>42.132</td>\n",
       "      <td>32.570</td>\n",
       "      <td>...</td>\n",
       "      <td>43231.286</td>\n",
       "      <td>163659.229</td>\n",
       "      <td>850.869</td>\n",
       "      <td>-21136.699</td>\n",
       "      <td>-367.122</td>\n",
       "      <td>1798.703</td>\n",
       "      <td>44681.209</td>\n",
       "      <td>362.168</td>\n",
       "      <td>317.901</td>\n",
       "      <td>7.845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 802 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0            AW  AWeight   Arto  BertzCT    Chi0    Chi1  \\\n",
       "0  NCGC00178831-03  5.436720e+07   13.053  2.176    3.194  23.112  15.868   \n",
       "1  NCGC00166114-03  1.268818e+07   22.123  2.065    3.137  21.033  13.718   \n",
       "2  NCGC00263563-01  3.076932e+06   13.085  2.154    3.207  46.896  29.958   \n",
       "3  NCGC00013058-02  7.168569e+07   12.832  2.029    3.380  51.086  32.045   \n",
       "4  NCGC00167516-01  7.989702e+06   12.936  2.124    3.573  70.295  46.402   \n",
       "\n",
       "   Chi10    Chi2    Chi3  ...        W3D        W3DH    WNSA1      WNSA2  \\\n",
       "0  1.496  15.127  12.592  ...   2687.469    9241.018  115.371   -915.496   \n",
       "1  1.937  13.187  11.951  ...   2184.384    3234.199  194.740  -1029.609   \n",
       "2  3.806  30.105  25.569  ...  13803.524   76582.899  238.004  -4358.946   \n",
       "3  1.806  29.090  21.603  ...  13807.345   50498.175  226.312  -2785.555   \n",
       "4  3.604  42.132  32.570  ...  43231.286  163659.229  850.869 -21136.699   \n",
       "\n",
       "     WNSA3     WPSA1      WPSA2    WPSA3     grav   rygr  \n",
       "0  -39.983   290.078   2301.941   59.492   88.147  3.708  \n",
       "1  -34.205   235.360   1244.323   82.906  134.852  4.131  \n",
       "2 -106.537   868.685  15909.444  135.335  216.852  5.075  \n",
       "3  -61.923   763.288   9394.859  125.509  238.265  4.640  \n",
       "4 -367.122  1798.703  44681.209  362.168  317.901  7.845  \n",
       "\n",
       "[5 rows x 802 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Masks for Loss evaluation\n",
    "The masks below {add explanation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0  NR.AhR  NR.AR  NR.AR.LBD  NR.Aromatase  NR.ER  NR.ER.LBD  \\\n",
      "0            True   False  False      False         False  False      False   \n",
      "1            True   False  False      False         False  False      False   \n",
      "2            True   False  False      False         False  False      False   \n",
      "3            True   False  False      False         False  False      False   \n",
      "4            True   False   True      False         False  False      False   \n",
      "...           ...     ...    ...        ...           ...    ...        ...   \n",
      "12055        True    True   True       True          True   True       True   \n",
      "12056        True    True   True       True         False  False       True   \n",
      "12057        True    True   True       True          True   True       True   \n",
      "12058        True    True   True       True          True   True       True   \n",
      "12059        True    True   True       True          True  False       True   \n",
      "\n",
      "       NR.PPAR.gamma  SR.ARE  SR.ATAD5  SR.HSE  SR.MMP  SR.p53  \n",
      "0              False   False     False    True   False   False  \n",
      "1              False   False     False    True   False   False  \n",
      "2              False   False     False    True   False   False  \n",
      "3              False   False     False    True   False   False  \n",
      "4              False   False     False   False   False   False  \n",
      "...              ...     ...       ...     ...     ...     ...  \n",
      "12055           True    True      True    True    True    True  \n",
      "12056           True   False     False   False   False   False  \n",
      "12057           True    True      True    True    True    True  \n",
      "12058           True    True      True    True    True    True  \n",
      "12059           True    True      True    True    True    True  \n",
      "\n",
      "[12060 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "y_train_mask = ~y_train.isna()\n",
    "print(y_train_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multitask_model(input_dim, num_tasks, hidden_units=1024, num_layers=2, dropout_rate=0.5, \n",
    "                           input_dropout=0.2, learning_rate=0.01, l2_weight_decay=1e-5, normalization='standard-deviation',\n",
    "                           loss_function='binary_crossentropy'):\n",
    "    \n",
    "    # Normalize input data based on specified normalization\n",
    "    if normalization == 'standard-deviation':\n",
    "        scaler = StandardScaler()\n",
    "    elif normalization == 'tanh':\n",
    "        # Tanh normalization can be done within the model using a tanh activation layer.\n",
    "        pass\n",
    "    elif normalization == 'sqrt':\n",
    "        pass\n",
    "\n",
    "    # Input layer\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    \n",
    "    # Optional input dropout\n",
    "    x = Dropout(input_dropout)(inputs) if input_dropout > 0 else inputs\n",
    "\n",
    "    # Shared hidden layers with L2 regularization and dropout\n",
    "    for _ in range(num_layers):\n",
    "        x = Dense(hidden_units, activation='relu', kernel_regularizer=l2(l2_weight_decay))(x)\n",
    "        if dropout_rate > 0:\n",
    "            x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    # Task-specific output layers\n",
    "    outputs = []\n",
    "    for _ in range(num_tasks):\n",
    "        task_output = Dense(1, activation='sigmoid')(x)\n",
    "        outputs.append(task_output)\n",
    "\n",
    "    # Define the model with shared input and multiple task-specific outputs\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # Define the optimizer with the specified learning rate\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    # Compile model with binary cross-entropy loss for each task\n",
    "    model.compile(optimizer=optimizer, loss=loss_function)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x_train \u001b[38;5;241m=\u001b[39m \u001b[43mx_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m[:, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m      2\u001b[0m y_train \u001b[38;5;241m=\u001b[39m y_train\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m      3\u001b[0m y_train_mask \u001b[38;5;241m=\u001b[39m y_train_mask\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mvalues\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "x_train = x_train.iloc[:, 1:].values\n",
    "y_train = y_train.iloc[:, 1:].values\n",
    "y_train_mask = y_train_mask.iloc[:, 1:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_binary_crossentropy(y_true, y_pred, mask):\n",
    "    epsilon = 1e-7  # To avoid log(0)\n",
    "    y_pred = K.clip(y_pred, epsilon, 1.0 - epsilon)  # Clip y_pred to avoid log(0)\n",
    "    \n",
    "    # Calculate cross-entropy loss\n",
    "    loss = - (y_true * K.log(y_pred) + (1 - y_true) * K.log(1 - y_pred))\n",
    "\n",
    "    # Ensure mask is float32 to allow for reduction operations\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    \n",
    "    # Apply the mask to include only valid entries\n",
    "    masked_loss = loss * mask\n",
    "    \n",
    "    # Normalize by the number of valid (non-masked) entries to get the average loss\n",
    "    return K.sum(masked_loss) / K.sum(mask)  # Return mean of masked loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_with_mask = np.concatenate([y_train, y_train_mask], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = x_train.shape[0]\n",
    "NUM_FEATURES = x_train.shape[1]\n",
    "NUM_TASKS = y_train.shape[1]\n",
    "\n",
    "HIDDEN_UNITS = 1024\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT_RATE = 0.5\n",
    "INPUT_DROPOUT = 0.2\n",
    "LEARNING_RATE = 0.01\n",
    "L2_WEIGHT_DECAY = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_7\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_7\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_7       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">801</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_21          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">801</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_98 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">821,248</span> │ dropout_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_22          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_98[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_99 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,049,600</span> │ dropout_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_23          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_99[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_100 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,025</span> │ dropout_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_101 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,025</span> │ dropout_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_102 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,025</span> │ dropout_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_103 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,025</span> │ dropout_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_104 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,025</span> │ dropout_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_105 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,025</span> │ dropout_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_106 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,025</span> │ dropout_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_107 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,025</span> │ dropout_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_108 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,025</span> │ dropout_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_109 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,025</span> │ dropout_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_110 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,025</span> │ dropout_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_111 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,025</span> │ dropout_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_7       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m801\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_21          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m801\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ input_layer_7[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_98 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      │    \u001b[38;5;34m821,248\u001b[0m │ dropout_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_22          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ dense_98[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_99 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      │  \u001b[38;5;34m1,049,600\u001b[0m │ dropout_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_23          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ dense_99[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_100 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │      \u001b[38;5;34m1,025\u001b[0m │ dropout_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_101 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │      \u001b[38;5;34m1,025\u001b[0m │ dropout_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_102 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │      \u001b[38;5;34m1,025\u001b[0m │ dropout_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_103 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │      \u001b[38;5;34m1,025\u001b[0m │ dropout_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_104 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │      \u001b[38;5;34m1,025\u001b[0m │ dropout_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_105 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │      \u001b[38;5;34m1,025\u001b[0m │ dropout_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_106 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │      \u001b[38;5;34m1,025\u001b[0m │ dropout_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_107 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │      \u001b[38;5;34m1,025\u001b[0m │ dropout_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_108 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │      \u001b[38;5;34m1,025\u001b[0m │ dropout_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_109 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │      \u001b[38;5;34m1,025\u001b[0m │ dropout_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_110 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │      \u001b[38;5;34m1,025\u001b[0m │ dropout_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_111 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │      \u001b[38;5;34m1,025\u001b[0m │ dropout_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,883,148</span> (7.18 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,883,148\u001b[0m (7.18 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,883,148</span> (7.18 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,883,148\u001b[0m (7.18 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = create_multitask_model(input_dim=NUM_FEATURES, num_tasks=NUM_TASKS, \n",
    "                               hidden_units=HIDDEN_UNITS, num_layers=NUM_LAYERS,\n",
    "                               dropout_rate=DROPOUT_RATE, input_dropout=INPUT_DROPOUT, \n",
    "                               learning_rate=LEARNING_RATE, l2_weight_decay=L2_WEIGHT_DECAY, \n",
    "                               loss_function=masked_binary_crossentropy)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1, Loss: nan\n",
      "Batch 2, Loss: nan\n",
      "Batch 3, Loss: nan\n",
      "Batch 4, Loss: nan\n",
      "Batch 5, Loss: nan\n",
      "Batch 6, Loss: nan\n",
      "Batch 7, Loss: nan\n",
      "Batch 8, Loss: nan\n",
      "Batch 9, Loss: nan\n",
      "Batch 10, Loss: nan\n",
      "Batch 11, Loss: nan\n",
      "Batch 12, Loss: nan\n",
      "Batch 13, Loss: nan\n",
      "Batch 14, Loss: nan\n",
      "Batch 15, Loss: nan\n",
      "Batch 16, Loss: nan\n",
      "Batch 17, Loss: nan\n",
      "Batch 18, Loss: nan\n",
      "Batch 19, Loss: nan\n",
      "Batch 20, Loss: nan\n",
      "Batch 21, Loss: nan\n",
      "Batch 22, Loss: nan\n",
      "Batch 23, Loss: nan\n",
      "Batch 24, Loss: nan\n",
      "Batch 25, Loss: nan\n",
      "Batch 26, Loss: nan\n",
      "Batch 27, Loss: nan\n",
      "Batch 28, Loss: nan\n",
      "Batch 29, Loss: nan\n",
      "Batch 30, Loss: nan\n",
      "Batch 31, Loss: nan\n",
      "Batch 32, Loss: nan\n",
      "Batch 33, Loss: nan\n",
      "Batch 34, Loss: nan\n",
      "Batch 35, Loss: nan\n",
      "Batch 36, Loss: nan\n",
      "Batch 37, Loss: nan\n",
      "Batch 38, Loss: nan\n",
      "Batch 39, Loss: nan\n",
      "Batch 40, Loss: nan\n",
      "Batch 41, Loss: nan\n",
      "Batch 42, Loss: nan\n",
      "Batch 43, Loss: nan\n",
      "Batch 44, Loss: nan\n",
      "Batch 45, Loss: nan\n",
      "Batch 46, Loss: nan\n",
      "Batch 47, Loss: nan\n",
      "Batch 48, Loss: nan\n",
      "Batch 49, Loss: nan\n",
      "Batch 50, Loss: nan\n",
      "Batch 51, Loss: nan\n",
      "Batch 52, Loss: nan\n",
      "Batch 53, Loss: nan\n",
      "Batch 54, Loss: nan\n",
      "Batch 55, Loss: nan\n",
      "Batch 56, Loss: nan\n",
      "Batch 57, Loss: nan\n",
      "Batch 58, Loss: nan\n",
      "Batch 59, Loss: nan\n",
      "Batch 60, Loss: nan\n",
      "Batch 61, Loss: nan\n",
      "Batch 62, Loss: nan\n",
      "Batch 63, Loss: nan\n",
      "Batch 64, Loss: nan\n",
      "Batch 65, Loss: nan\n",
      "Batch 66, Loss: nan\n",
      "Batch 67, Loss: nan\n",
      "Batch 68, Loss: nan\n",
      "Batch 69, Loss: nan\n",
      "Batch 70, Loss: nan\n",
      "Batch 71, Loss: nan\n",
      "Batch 72, Loss: nan\n",
      "Batch 73, Loss: nan\n",
      "Batch 74, Loss: nan\n",
      "Batch 75, Loss: nan\n",
      "Batch 76, Loss: nan\n",
      "Batch 77, Loss: nan\n",
      "Batch 78, Loss: nan\n",
      "Batch 79, Loss: nan\n",
      "Batch 80, Loss: nan\n",
      "Batch 81, Loss: nan\n",
      "Batch 82, Loss: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m         loss \u001b[38;5;241m=\u001b[39m masked_binary_crossentropy(y_batch, y_pred, mask_batch)\n\u001b[1;32m     18\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, model\u001b[38;5;241m.\u001b[39mtrainable_variables)\n\u001b[0;32m---> 19\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgradients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mbatch_size\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnd of Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/keras/src/optimizers/base_optimizer.py:344\u001b[0m, in \u001b[0;36mBaseOptimizer.apply_gradients\u001b[0;34m(self, grads_and_vars)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_gradients\u001b[39m(\u001b[38;5;28mself\u001b[39m, grads_and_vars):\n\u001b[1;32m    343\u001b[0m     grads, trainable_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mgrads_and_vars)\n\u001b[0;32m--> 344\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;66;03m# Return iterations for compat with tf.keras.\u001b[39;00m\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterations\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/keras/src/optimizers/base_optimizer.py:409\u001b[0m, in \u001b[0;36mBaseOptimizer.apply\u001b[0;34m(self, grads, trainable_variables)\u001b[0m\n\u001b[1;32m    406\u001b[0m     grads \u001b[38;5;241m=\u001b[39m [g \u001b[38;5;28;01mif\u001b[39;00m g \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m g \u001b[38;5;241m/\u001b[39m scale \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m grads]\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# Apply gradient updates.\u001b[39;00m\n\u001b[0;32m--> 409\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend_apply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;66;03m# Apply variable constraints after applying gradients.\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m variable \u001b[38;5;129;01min\u001b[39;00m trainable_variables:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/keras/src/optimizers/base_optimizer.py:472\u001b[0m, in \u001b[0;36mBaseOptimizer._backend_apply_gradients\u001b[0;34m(self, grads, trainable_variables)\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_weight_decay(trainable_variables)\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;66;03m# Run udpate step.\u001b[39;00m\n\u001b[0;32m--> 472\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend_update_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_ema:\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_variables_moving_average(\n\u001b[1;32m    478\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainable_variables\n\u001b[1;32m    479\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/keras/src/backend/tensorflow/optimizer.py:122\u001b[0m, in \u001b[0;36mTFOptimizer._backend_update_step\u001b[0;34m(self, grads, trainable_variables, learning_rate)\u001b[0m\n\u001b[1;32m    120\u001b[0m grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(grads, trainable_variables))\n\u001b[1;32m    121\u001b[0m grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_all_reduce_sum_gradients(grads_and_vars)\n\u001b[0;32m--> 122\u001b[0m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__internal__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaybe_merge_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distributed_tf_update_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distribution_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/tensorflow/python/distribute/merge_call_interim.py:51\u001b[0m, in \u001b[0;36mmaybe_merge_call\u001b[0;34m(fn, strategy, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Maybe invoke `fn` via `merge_call` which may or may not be fulfilled.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03mThe caller of this utility function requests to invoke `fn` via `merge_call`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03m  The return value of the `fn` call.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strategy_supports_no_merge_call():\n\u001b[0;32m---> 51\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m distribute_lib\u001b[38;5;241m.\u001b[39mget_replica_context()\u001b[38;5;241m.\u001b[39mmerge_call(\n\u001b[1;32m     54\u001b[0m       fn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/keras/src/backend/tensorflow/optimizer.py:136\u001b[0m, in \u001b[0;36mTFOptimizer._distributed_tf_update_step\u001b[0;34m(self, distribution, grads_and_vars, learning_rate)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_step(grad, var, learning_rate)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m grad, var \u001b[38;5;129;01min\u001b[39;00m grads_and_vars:\n\u001b[0;32m--> 136\u001b[0m     \u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextended\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapply_grad_to_update_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/tensorflow/python/distribute/distribute_lib.py:3005\u001b[0m, in \u001b[0;36mStrategyExtendedV2.update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   3002\u001b[0m   fn \u001b[38;5;241m=\u001b[39m autograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[1;32m   3003\u001b[0m       fn, autograph_ctx\u001b[38;5;241m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   3004\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy()\u001b[38;5;241m.\u001b[39mscope():\n\u001b[0;32m-> 3005\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3006\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3007\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_replica_ctx_update(\n\u001b[1;32m   3008\u001b[0m       var, fn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs, group\u001b[38;5;241m=\u001b[39mgroup)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/tensorflow/python/distribute/distribute_lib.py:4075\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   4072\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update\u001b[39m(\u001b[38;5;28mself\u001b[39m, var, fn, args, kwargs, group):\n\u001b[1;32m   4073\u001b[0m   \u001b[38;5;66;03m# The implementations of _update() and _update_non_slot() are identical\u001b[39;00m\n\u001b[1;32m   4074\u001b[0m   \u001b[38;5;66;03m# except _update() passes `var` as the first argument to `fn()`.\u001b[39;00m\n\u001b[0;32m-> 4075\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_non_slot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/tensorflow/python/distribute/distribute_lib.py:4081\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update_non_slot\u001b[0;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[1;32m   4077\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_non_slot\u001b[39m(\u001b[38;5;28mself\u001b[39m, colocate_with, fn, args, kwargs, should_group):\n\u001b[1;32m   4078\u001b[0m   \u001b[38;5;66;03m# TODO(josh11b): Figure out what we should be passing to UpdateContext()\u001b[39;00m\n\u001b[1;32m   4079\u001b[0m   \u001b[38;5;66;03m# once that value is used for something.\u001b[39;00m\n\u001b[1;32m   4080\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m UpdateContext(colocate_with):\n\u001b[0;32m-> 4081\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4082\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_group:\n\u001b[1;32m   4083\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py:596\u001b[0m, in \u001b[0;36mcall_with_unspecified_conversion_status.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    595\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx\u001b[38;5;241m.\u001b[39mControlStatusCtx(status\u001b[38;5;241m=\u001b[39mag_ctx\u001b[38;5;241m.\u001b[39mStatus\u001b[38;5;241m.\u001b[39mUNSPECIFIED):\n\u001b[0;32m--> 596\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/keras/src/backend/tensorflow/optimizer.py:133\u001b[0m, in \u001b[0;36mTFOptimizer._distributed_tf_update_step.<locals>.apply_grad_to_update_var\u001b[0;34m(var, grad, learning_rate)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_grad_to_update_var\u001b[39m(var, grad, learning_rate):\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/keras/src/optimizers/adam.py:148\u001b[0m, in \u001b[0;36mAdam.update_step\u001b[0;34m(self, gradient, variable, learning_rate)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massign(v_hat, ops\u001b[38;5;241m.\u001b[39mmaximum(v_hat, v))\n\u001b[1;32m    144\u001b[0m     v \u001b[38;5;241m=\u001b[39m v_hat\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massign_sub(\n\u001b[1;32m    146\u001b[0m     variable,\n\u001b[1;32m    147\u001b[0m     ops\u001b[38;5;241m.\u001b[39mdivide(\n\u001b[0;32m--> 148\u001b[0m         \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m, ops\u001b[38;5;241m.\u001b[39madd(ops\u001b[38;5;241m.\u001b[39msqrt(v), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon)\n\u001b[1;32m    149\u001b[0m     ),\n\u001b[1;32m    150\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/keras/src/ops/numpy.py:5837\u001b[0m, in \u001b[0;36mmultiply\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m   5835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors((x1, x2)):\n\u001b[1;32m   5836\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Multiply()\u001b[38;5;241m.\u001b[39msymbolic_call(x1, x2)\n\u001b[0;32m-> 5837\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/keras/src/backend/tensorflow/sparse.py:627\u001b[0m, in \u001b[0;36melementwise_binary_intersection.<locals>.sparse_wrapper\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m    621\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mIndexedSlices(\n\u001b[1;32m    622\u001b[0m             func(tf\u001b[38;5;241m.\u001b[39mgather(x1, x2\u001b[38;5;241m.\u001b[39mindices), x2\u001b[38;5;241m.\u001b[39mvalues),\n\u001b[1;32m    623\u001b[0m             x2\u001b[38;5;241m.\u001b[39mindices,\n\u001b[1;32m    624\u001b[0m             x2\u001b[38;5;241m.\u001b[39mdense_shape,\n\u001b[1;32m    625\u001b[0m         )\n\u001b[1;32m    626\u001b[0m \u001b[38;5;66;03m# Default case, no SparseTensor and no IndexedSlices.\u001b[39;00m\n\u001b[0;32m--> 627\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/keras/src/backend/tensorflow/numpy.py:499\u001b[0m, in \u001b[0;36mmultiply\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m    497\u001b[0m x1 \u001b[38;5;241m=\u001b[39m convert_to_tensor(x1, dtype)\n\u001b[1;32m    498\u001b[0m x2 \u001b[38;5;241m=\u001b[39m convert_to_tensor(x2, dtype)\n\u001b[0;32m--> 499\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142\u001b[0m, in \u001b[0;36mweak_tensor_binary_op_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    141\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    144\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1260\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1260\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1262\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1263\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1264\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/tensorflow/python/ops/math_ops.py:526\u001b[0m, in \u001b[0;36mmultiply\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmath.multiply\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    478\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39mregister_binary_elementwise_api\n\u001b[1;32m    479\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmultiply\u001b[39m(x, y, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    481\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Returns an element-wise x * y.\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \n\u001b[1;32m    483\u001b[0m \u001b[38;5;124;03m  For example:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;124;03m   * InvalidArgumentError: When `x` and `y` have incompatible shapes or types.\u001b[39;00m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 526\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_math_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/tensorflow/python/ops/gen_math_ops.py:6832\u001b[0m, in \u001b[0;36mmul\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   6830\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   6831\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 6832\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmul_eager_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   6833\u001b[0m \u001b[43m      \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6834\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_SymbolicException:\n\u001b[1;32m   6835\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/tensorflow/python/ops/gen_math_ops.py:6852\u001b[0m, in \u001b[0;36mmul_eager_fallback\u001b[0;34m(x, y, name, ctx)\u001b[0m\n\u001b[1;32m   6851\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmul_eager_fallback\u001b[39m(x: Annotated[Any, TV_Mul_T], y: Annotated[Any, TV_Mul_T], name, ctx) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Annotated[Any, TV_Mul_T]:\n\u001b[0;32m-> 6852\u001b[0m   _attr_T, _inputs_T \u001b[38;5;241m=\u001b[39m \u001b[43m_execute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs_to_matching_eager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhalf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint8\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomplex64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomplex128\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6853\u001b[0m   (x, y) \u001b[38;5;241m=\u001b[39m _inputs_T\n\u001b[1;32m   6854\u001b[0m   _inputs_flat \u001b[38;5;241m=\u001b[39m [x, y]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:267\u001b[0m, in \u001b[0;36margs_to_matching_eager\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    265\u001b[0m       dtype \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 267\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mtensor_conversion_registry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): consider removing this as it leaks a Keras concept.\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    271\u001b[0m keras_symbolic_tensors \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m ret \u001b[38;5;28;01mif\u001b[39;00m _is_keras_symbolic_tensor(x)]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:267\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    265\u001b[0m       dtype \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 267\u001b[0m   ret \u001b[38;5;241m=\u001b[39m [\u001b[43mtensor_conversion_registry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m l]\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): consider removing this as it leaks a Keras concept.\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    271\u001b[0m keras_symbolic_tensors \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m ret \u001b[38;5;28;01mif\u001b[39;00m _is_keras_symbolic_tensor(x)]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:164\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_add_error_prefix\u001b[39m(msg, \u001b[38;5;241m*\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    161\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m msg \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(value,\n\u001b[1;32m    165\u001b[0m             dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    166\u001b[0m             name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    167\u001b[0m             as_ref\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    168\u001b[0m             preferred_dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    169\u001b[0m             accepted_result_types\u001b[38;5;241m=\u001b[39m(core\u001b[38;5;241m.\u001b[39mSymbol,)):\n\u001b[1;32m    170\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Converts `value` to a `Tensor` using registered conversion functions.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;124;03m      value.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m    202\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Number of epochs and batch size\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "# Start training\n",
    "# Custom training loop\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    for i in range(0, len(x_train), batch_size):\n",
    "        x_batch = x_train[i:i + batch_size]\n",
    "        y_batch = y_train[i:i + batch_size]\n",
    "        mask_batch = y_train_mask[i:i + batch_size]\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(x_batch, training=True)\n",
    "            loss = masked_binary_crossentropy(y_batch, y_pred, mask_batch)\n",
    "        \n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        print(f\"Batch {i // batch_size + 1}, Loss: {loss.numpy()}\")\n",
    "\n",
    "    print(f\"End of Epoch {epoch + 1}, Loss: {loss.numpy()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12060, 12)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
